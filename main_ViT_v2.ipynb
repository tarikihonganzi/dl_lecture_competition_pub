{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "from statistics import mode\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import rearrange\n",
    "from einops import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 数詞を数字に変換\n",
    "    num_word_to_digit = {\n",
    "        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n",
    "        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n",
    "        'ten': '10'\n",
    "    }\n",
    "    for word, digit in num_word_to_digit.items():\n",
    "        text = text.replace(word, digit)\n",
    "\n",
    "    # 小数点のピリオドを削除\n",
    "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n",
    "\n",
    "    # 冠詞の削除\n",
    "    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n",
    "\n",
    "    # 短縮形のカンマの追加\n",
    "    contractions = {\n",
    "        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\n",
    "        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\"\n",
    "    }\n",
    "    for contraction, correct in contractions.items():\n",
    "        text = text.replace(contraction, correct)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "\n",
    "    # 連続するスペースを1つに変換\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. データローダーの作成\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_path, image_dir, transform=None, answer=True):\n",
    "        self.transform = transform  # 画像の前処理\n",
    "        self.image_dir = image_dir  # 画像ファイルのディレクトリ\n",
    "        self.df = pandas.read_json(df_path)  # 画像ファイルのパス，question, answerを持つDataFrame\n",
    "        self.answer = answer\n",
    "\n",
    "        # question / answerの辞書を作成\n",
    "        self.question2idx = {}\n",
    "        self.answer2idx = {}\n",
    "        self.idx2question = {}\n",
    "        self.idx2answer = {}\n",
    "\n",
    "        # 質問文に含まれる単語を辞書に追加\n",
    "        for question in self.df[\"question\"]:\n",
    "            question = process_text(question)\n",
    "            words = question.split(\" \")\n",
    "            for word in words:\n",
    "                if word not in self.question2idx:\n",
    "                    self.question2idx[word] = len(self.question2idx)\n",
    "        self.idx2question = {v: k for k, v in self.question2idx.items()}  # 逆変換用の辞書(question)\n",
    "\n",
    "        if self.answer:\n",
    "            # 回答に含まれる単語を辞書に追加\n",
    "            for answers in self.df[\"answers\"]:\n",
    "                for answer in answers:\n",
    "                    word = answer[\"answer\"]\n",
    "                    word = process_text(word)\n",
    "                    if word not in self.answer2idx:\n",
    "                        self.answer2idx[word] = len(self.answer2idx)\n",
    "            self.idx2answer = {v: k for k, v in self.answer2idx.items()}  # 逆変換用の辞書(answer)\n",
    "\n",
    "    def update_dict(self, dataset):\n",
    "        \"\"\"\n",
    "        検証用データ，テストデータの辞書を訓練データの辞書に更新する．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : Dataset\n",
    "            訓練データのDataset\n",
    "        \"\"\"\n",
    "        self.question2idx = dataset.question2idx\n",
    "        self.answer2idx = dataset.answer2idx\n",
    "        self.idx2question = dataset.idx2question\n",
    "        self.idx2answer = dataset.idx2answer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        対応するidxのデータ（画像，質問，回答）を取得．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            取得するデータのインデックス\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        image : torch.Tensor  (C, H, W)\n",
    "            画像データ\n",
    "        question : torch.Tensor  (vocab_size)\n",
    "            質問文をone-hot表現に変換したもの\n",
    "        answers : torch.Tensor  (n_answer)\n",
    "            10人の回答者の回答のid\n",
    "        mode_answer_idx : torch.Tensor  (1)\n",
    "            10人の回答者の回答の中で最頻値の回答のid\n",
    "        \"\"\"\n",
    "        image = Image.open(f\"{self.image_dir}/{self.df['image'][idx]}\")\n",
    "        image = self.transform(image)\n",
    "        question = np.zeros(len(self.idx2question) + 1)  # 未知語用の要素を追加\n",
    "        question_words = self.df[\"question\"][idx].split(\" \")\n",
    "        for word in question_words:\n",
    "            try:\n",
    "                question[self.question2idx[word]] = 1  # one-hot表現に変換\n",
    "            except KeyError:\n",
    "                question[-1] = 1  # 未知語\n",
    "\n",
    "        if self.answer:\n",
    "            answers = [self.answer2idx[process_text(answer[\"answer\"])] for answer in self.df[\"answers\"][idx]]\n",
    "            mode_answer_idx = mode(answers)  # 最頻値を取得（正解ラベル）\n",
    "\n",
    "            return image, torch.Tensor(question), torch.Tensor(answers), int(mode_answer_idx)\n",
    "\n",
    "        else:\n",
    "            return image, torch.Tensor(question)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 評価指標の実装\n",
    "# 簡単にするならBCEを利用する\n",
    "def VQA_criterion(batch_pred: torch.Tensor, batch_answers: torch.Tensor):\n",
    "    total_acc = 0.\n",
    "\n",
    "    for pred, answers in zip(batch_pred, batch_answers):\n",
    "        acc = 0.\n",
    "        for i in range(len(answers)):\n",
    "            num_match = 0\n",
    "            for j in range(len(answers)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if pred == answers[j]:\n",
    "                    num_match += 1\n",
    "            acc += min(num_match / 3, 1)\n",
    "        total_acc += acc / 10\n",
    "\n",
    "    return total_acc / len(batch_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attentionの実装\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads, dim_head, dropout=0.):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        dim : int\n",
    "            入力データの次元数．埋め込み次元数と一致する．\n",
    "        heads : int\n",
    "            ヘッドの数．\n",
    "        dim_head : int\n",
    "            各ヘッドのデータの次元数．\n",
    "        dropout : float\n",
    "            Dropoutの確率(default=0.)．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.dim_head = dim_head\n",
    "        inner_dim = dim_head * heads  # ヘッドに分割する前のQ, K, Vの次元数．self.dimと異なっても良い．\n",
    "        project_out = not (heads == 1 and dim_head == dim)  # headsが1，dim_headがdimと等しければ通常のSelf-Attention\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = math.sqrt(dim_head)  # ソフトマックス関数を適用する前のスケーリング係数(dim_k)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)  # アテンションスコアの算出に利用するソフトマックス関数\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Q, K, Vに変換するための全結合層\n",
    "        self.to_q = nn.Linear(in_features=dim, out_features=inner_dim)\n",
    "        self.to_k = nn.Linear(in_features=dim, out_features=inner_dim)\n",
    "        self.to_v = nn.Linear(in_features=dim, out_features=inner_dim)\n",
    "\n",
    "        # dim != inner_dimなら線形層を入れる，そうでなければそのまま出力\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(in_features=inner_dim, out_features=dim),\n",
    "            nn.Dropout(dropout),\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        B: バッチサイズ\n",
    "        N: 系列長\n",
    "        D: データの次元数(dim)\n",
    "        \"\"\"\n",
    "        B, N, D = x.size()\n",
    "\n",
    "        # 入力データをQ, K, Vに変換する\n",
    "        # (B, N, dim) -> (B, N, inner_dim)\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(x)\n",
    "\n",
    "        # Q, K, Vをヘッドに分割する\n",
    "        # (B, N, inner_dim) -> (B, heads, N, dim_head)\n",
    "        q = rearrange(q, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n",
    "        k = rearrange(k, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n",
    "        v = rearrange(v, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n",
    "\n",
    "        # QK^T / sqrt(d_k)を計算する\n",
    "        # (B, heads, N, dim_head) x (B, heads, dim_head, N) -> (B, heads, N, N)\n",
    "        dots = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        # ソフトマックス関数でスコアを算出し，Dropoutをする\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # softmax(QK^T / sqrt(d_k))Vを計算する\n",
    "        # (B, heads, N, N) x (B, heads, N, dim_head) -> (B, heads, N, dim_head)\n",
    "        out = torch.matmul(attn ,v)\n",
    "\n",
    "        # もとの形に戻す\n",
    "        # (B, heads, N, dim_head) -> (B, N, dim)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\", h=self.heads, d=self.dim_head)\n",
    "\n",
    "        # 次元が違っていればもとに戻して出力\n",
    "        # 表現の可視化のためにattention mapも返すようにしておく\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed-Forward Networkの実装\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        dim : int\n",
    "            入力データの次元数．\n",
    "        hidden_dim : int\n",
    "            隠れ層の次元．\n",
    "        dropout : float\n",
    "            各全結合層の後のDropoutの確率(default=0.)．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=dim, out_features=hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        (B, D) -> (B, D)\n",
    "        B: バッチサイズ\n",
    "        D: 次元数\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, heads, dim_head, mlp_dim, dropout):\n",
    "        \"\"\"\n",
    "        TransformerのEncoder Blockの実装．\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        dim : int\n",
    "            埋め込みされた次元数．PatchEmbedのembed_dimと同じ値．\n",
    "        heads : int\n",
    "            Multi-Head Attentionのヘッドの数．\n",
    "        dim_head : int\n",
    "            Multi-Head Attentionの各ヘッドの次元数．\n",
    "        mlp_dim : int\n",
    "            Feed-Forward Networkの隠れ層の次元数．\n",
    "        dropout : float\n",
    "            Droptou層の確率p．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn_ln = nn.LayerNorm(dim)  # Attention前のLayerNorm\n",
    "        self.attn = Attention(dim, heads, dim_head, dropout)\n",
    "        self.ffn_ln = nn.LayerNorm(dim)  # FFN前のLayerNorm\n",
    "        self.ffn = FFN(dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "        \"\"\"\n",
    "        x: (B, N, dim)\n",
    "        B: バッチサイズ\n",
    "        N: 系列長\n",
    "        dim: 埋め込み次元\n",
    "        \"\"\"\n",
    "        y = self.attn(self.attn_ln(x))\n",
    "        x = y + x\n",
    "        out = self.ffn(self.ffn_ln(x)) + x\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch Embeddingの実装\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
    "        \"\"\"\n",
    "        入力画像をパッチごとに埋め込むための層．\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        image_size : Tuple[int]\n",
    "            入力画像のサイズ．\n",
    "        patch_size : Tuple[int]\n",
    "            各パッチのサイズ．\n",
    "        in_channels : int\n",
    "            入力画像のチャネル数．\n",
    "        embed_dim : int\n",
    "            埋め込み後の次元数．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        image_height, image_width = image_size\n",
    "        patch_height, patch_width = patch_size\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, \"パッチサイズは，入力画像のサイズを割り切れる必要があります．\"\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)  # パッチの数\n",
    "        patch_dim = in_channels * patch_height * patch_width  # 各パッチを平坦化したときの次元数\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=patch_height, p2=patch_width),  # 画像をパッチに分割して平坦化\n",
    "            nn.Linear(in_features=patch_dim, out_features=embed_dim),  # 埋め込みを行う\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        B: バッチサイズ\n",
    "        C: 入力画像のチャネル数\n",
    "        H: 入力画像の高さ\n",
    "        W: 入力画像の幅\n",
    "        \"\"\"\n",
    "        return self.to_patch_embedding(x)  # (B, C, H, W) -> (B, num_patches, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, image_size=[32, 32], patch_size=[2, 2], emb_dim=192, num_layer=12,\n",
    "                 heads=3, dim_head=64, mlp_dim=192, class_num = 20, dropout=0.):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "\n",
    "        image_size : List[int]\n",
    "            入力画像の大きさ．\n",
    "        patch_size : List[int]\n",
    "            各パッチの大きさ．\n",
    "        emb_dim : int\n",
    "            データを埋め込む次元の数．\n",
    "        num_layer : int\n",
    "            Encoderに含まれるBlockの数．\n",
    "        heads : int\n",
    "            Multi-Head Attentionのヘッドの数．\n",
    "        dim_head : int\n",
    "            Multi-Head Attentionの各ヘッドの次元数．\n",
    "        mlp_dim : int\n",
    "            Feed-Forward Networkの隠れ層の次元数．\n",
    "        class_num : int\n",
    "            分類クラウ数．\n",
    "        dropout : float\n",
    "            ドロップアウトの確率．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        img_height, img_width = image_size\n",
    "        patch_height, patch_width = patch_size\n",
    "        num_patches = (img_height // patch_height) * (img_width // patch_width)\n",
    "\n",
    "        self.cls_token = torch.nn.Parameter(torch.randn(1, 1, emb_dim))  # class tokenの初期化\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.randn(1, num_patches, emb_dim))  # positional embedding（学習可能にしている）\n",
    "\n",
    "        # 入力画像をパッチに分割する\n",
    "        self.patchify = PatchEmbedding(image_size, patch_size, 3, emb_dim)\n",
    "\n",
    "        # Encoder（Blockを重ねる）\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, heads, dim_head, mlp_dim, dropout) for _ in range(num_layer)])\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.mlpHead=nn.Linear(emb_dim, class_num)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        torch.nn.init.normal_(self.cls_token, std=0.02)\n",
    "        torch.nn.init.normal_(self.pos_embedding, std=0.02)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # 1. 入力画像をパッチに分割して，positional embeddingする\n",
    "        patches = self.patchify(img)\n",
    "        patches = patches + self.pos_embedding\n",
    "\n",
    "        # class tokenを結合\n",
    "        patches = torch.cat([self.cls_token.repeat(patches.shape[0], 1, 1), patches], dim=1)\n",
    "\n",
    "        # 3. Encoderで入力データを処理する\n",
    "        features = self.layer_norm(self.transformer(patches))\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine scheduler\n",
    "class CosineScheduler:\n",
    "    def __init__(self, epochs, lr, warmup_length=5):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        epochs : int\n",
    "            学習のエポック数．\n",
    "        lr : float\n",
    "            学習率．\n",
    "        warmup_length : int\n",
    "            warmupを適用するエポック数．\n",
    "        \"\"\"\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.warmup = warmup_length\n",
    "\n",
    "    def __call__(self, epoch):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        epoch : int\n",
    "            現在のエポック数．\n",
    "        \"\"\"\n",
    "        progress = (epoch - self.warmup) / (self.epochs - self.warmup)\n",
    "        progress = np.clip(progress, 0.0, 1.0)\n",
    "        lr = self.lr * 0.5 * (1. + np.cos(np.pi * progress))\n",
    "\n",
    "        if self.warmup:\n",
    "            lr = lr * min(1., (epoch+1) / self.warmup)\n",
    "\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lr(lr, optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "config = {\n",
    "    \"image_size\": [32, 32],\n",
    "    \"patch_size\": [2, 2],\n",
    "    \"emb_dim\": 192,\n",
    "    \"enc_layers\": 12,\n",
    "    \"enc_heads\": 3,\n",
    "    \"enc_dim_head\": 64,\n",
    "    \"enc_mlp_dim\": 192,\n",
    "    \"dec_layers\": 4,\n",
    "    \"dec_heads\": 3,\n",
    "    \"dec_dim_head\": 64,\n",
    "    \"dec_mlp_dim\": 192,\n",
    "    \"mask_ratio\": 0.75,\n",
    "    \"dropout\": 0.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Encoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "lr = 0.0024\n",
    "warmup_length = 200\n",
    "batch_size = 512\n",
    "step_count = 0\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95), weight_decay=0.05)\n",
    "scheduler = CosineScheduler(epochs, lr, warmup_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [01:01<00:00, 2769604.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./cifar-10-python.tar.gz to ./\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(0.5, 0.5)]\n",
    ")\n",
    "valid_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(0.5, 0.5)]\n",
    ")\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(\n",
    "        \"./\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=train_transform,\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "valid_dl = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(\n",
    "        \"./\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=valid_transform,\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rec_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m features \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m---> 16\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((\u001b[43mrec_img\u001b[49m \u001b[38;5;241m-\u001b[39m x) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m mask) \u001b[38;5;241m/\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask_ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     17\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m8\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# 8イテレーションごとに更新することで，擬似的にバッチサイズを大きくしている\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rec_img' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネル (Kernel) がクラッシュしました。\n",
      "\u001b[1;31mエラーの原因を特定するには、セル内のコードを確認してください。\n",
      "\u001b[1;31m詳細については<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a>をクリックします。\n",
      "\u001b[1;31m詳細については、Jupyter <a href='command:jupyter.viewOutput'>ログ</a> を参照してください。"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # スケジューラで学習率を更新する\n",
    "    new_lr = scheduler(epoch)\n",
    "    set_lr(new_lr, optimizer)\n",
    "\n",
    "    total_train_loss = 0.\n",
    "    total_valid_loss = 0.\n",
    "\n",
    "    # モデルの訓練\n",
    "    for x, _ in train_dl:\n",
    "        step_count += 1\n",
    "        model.train()\n",
    "        x = x.to(device)\n",
    "\n",
    "        features = model(x)\n",
    "        train_loss = torch.mean((rec_img - x) ** 2 * mask) / config[\"mask_ratio\"]\n",
    "        train_loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += train_loss.item()\n",
    "\n",
    "    # モデルの評価\n",
    "    with torch.no_grad():\n",
    "        for x, _ in valid_dl:\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                x = x.to(device)\n",
    "\n",
    "                rec_img, mask = model(x)\n",
    "                valid_loss = torch.mean((rec_img - x) ** 2 * mask) / config[\"mask_ratio\"]\n",
    "\n",
    "                total_valid_loss += valid_loss.item()\n",
    "\n",
    "\n",
    "    print(f\"Epoch[{epoch+1} / {epochs}] Train Loss: {total_train_loss/len(train_dl):.4f} Valid Loss: {total_valid_loss/len(valid_dl):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer / criterion\n",
    "num_epoch = 20\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# criterionをKL-Divergenceに変更\n",
    "# criterion = nn.KLDivLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 4])\n",
      "torch.Size([128, 9, 256])\n",
      "torch.Size([128, 10, 256])\n",
      "torch.Size([128, 4, 256])\n",
      "3\n",
      "torch.Size([128, 14, 256])\n",
      "4\n",
      "torch.Size([128, 14, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epoch):\n\u001b[1;32m----> 3\u001b[0m     train_loss, train_acc, train_simple_acc, train_time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m【\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m】\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [s]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain simple acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_simple_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     15\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(image, question)\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, mode_answer\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\ATSUHIRO\\anaconda3\\envs\\dl_competition\\lib\\site-packages\\torch\\_tensor.py:464\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    461\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[0;32m    462\u001b[0m     )\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ATSUHIRO\\anaconda3\\envs\\dl_competition\\lib\\site-packages\\torch\\_tensor_str.py:697\u001b[0m, in \u001b[0;36m_str\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[0;32m    696\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[1;32m--> 697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ATSUHIRO\\anaconda3\\envs\\dl_competition\\lib\\site-packages\\torch\\_tensor_str.py:617\u001b[0m, in \u001b[0;36m_str_intern\u001b[1;34m(inp, tensor_contents)\u001b[0m\n\u001b[0;32m    615\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[0;32m    616\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 617\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[0;32m    620\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[1;32mc:\\Users\\ATSUHIRO\\anaconda3\\envs\\dl_competition\\lib\\site-packages\\torch\\_tensor_str.py:349\u001b[0m, in \u001b[0;36m_tensor_str\u001b[1;34m(self, indent)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[0;32m    347\u001b[0m     )\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 349\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[1;32mc:\\Users\\ATSUHIRO\\anaconda3\\envs\\dl_competition\\lib\\site-packages\\torch\\_tensor_str.py:138\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_select(\n\u001b[1;32m--> 138\u001b[0m         tensor_view, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m tensor_view\u001b[38;5;241m.\u001b[39mne(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出用ファイルの作成\n",
    "model.eval()\n",
    "submission = []\n",
    "for image, question in test_loader:\n",
    "    image, question = image.to(device), question.to(device)\n",
    "    pred = model(image, question)\n",
    "    pred = pred.argmax(1).cpu().item()\n",
    "    submission.append(pred)\n",
    "\n",
    "submission = [train_dataset.idx2answer[id] for id in submission]\n",
    "submission = np.array(submission)\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "np.save(\"submission.npy\", submission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_competition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
