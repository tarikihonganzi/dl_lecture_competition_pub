{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "from statistics import mode\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import rearrange\n",
    "from einops import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 数詞を数字に変換\n",
    "    num_word_to_digit = {\n",
    "        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n",
    "        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n",
    "        'ten': '10'\n",
    "    }\n",
    "    for word, digit in num_word_to_digit.items():\n",
    "        text = text.replace(word, digit)\n",
    "\n",
    "    # 小数点のピリオドを削除\n",
    "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n",
    "\n",
    "    # 冠詞の削除\n",
    "    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n",
    "\n",
    "    # 短縮形のカンマの追加\n",
    "    contractions = {\n",
    "        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\n",
    "        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\"\n",
    "    }\n",
    "    for contraction, correct in contractions.items():\n",
    "        text = text.replace(contraction, correct)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "\n",
    "    # 連続するスペースを1つに変換\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. データローダーの作成\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_path, image_dir, transform=None, answer=True):\n",
    "        self.transform = transform  # 画像の前処理\n",
    "        self.image_dir = image_dir  # 画像ファイルのディレクトリ\n",
    "        self.df = pandas.read_json(df_path)  # 画像ファイルのパス，question, answerを持つDataFrame\n",
    "        self.answer = answer\n",
    "\n",
    "        # question / answerの辞書を作成\n",
    "        self.question2idx = {}\n",
    "        self.answer2idx = {}\n",
    "        self.idx2question = {}\n",
    "        self.idx2answer = {}\n",
    "\n",
    "        # 質問文に含まれる単語を辞書に追加\n",
    "        for question in self.df[\"question\"]:\n",
    "            question = process_text(question)\n",
    "            words = question.split(\" \")\n",
    "            for word in words:\n",
    "                if word not in self.question2idx:\n",
    "                    self.question2idx[word] = len(self.question2idx)\n",
    "        self.idx2question = {v: k for k, v in self.question2idx.items()}  # 逆変換用の辞書(question)\n",
    "\n",
    "        if self.answer:\n",
    "            # 回答に含まれる単語を辞書に追加\n",
    "            for answers in self.df[\"answers\"]:\n",
    "                for answer in answers:\n",
    "                    word = answer[\"answer\"]\n",
    "                    word = process_text(word)\n",
    "                    if word not in self.answer2idx:\n",
    "                        self.answer2idx[word] = len(self.answer2idx)\n",
    "            self.idx2answer = {v: k for k, v in self.answer2idx.items()}  # 逆変換用の辞書(answer)\n",
    "\n",
    "    def update_dict(self, dataset):\n",
    "        \"\"\"\n",
    "        検証用データ，テストデータの辞書を訓練データの辞書に更新する．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : Dataset\n",
    "            訓練データのDataset\n",
    "        \"\"\"\n",
    "        self.question2idx = dataset.question2idx\n",
    "        self.answer2idx = dataset.answer2idx\n",
    "        self.idx2question = dataset.idx2question\n",
    "        self.idx2answer = dataset.idx2answer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        対応するidxのデータ（画像，質問，回答）を取得．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            取得するデータのインデックス\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        image : torch.Tensor  (C, H, W)\n",
    "            画像データ\n",
    "        question : torch.Tensor  (vocab_size)\n",
    "            質問文をone-hot表現に変換したもの\n",
    "        answers : torch.Tensor  (n_answer)\n",
    "            10人の回答者の回答のid\n",
    "        mode_answer_idx : torch.Tensor  (1)\n",
    "            10人の回答者の回答の中で最頻値の回答のid\n",
    "        \"\"\"\n",
    "        image = Image.open(f\"{self.image_dir}/{self.df['image'][idx]}\")\n",
    "        image = self.transform(image)\n",
    "        question = np.zeros(len(self.idx2question) + 1)  # 未知語用の要素を追加\n",
    "        question_words = self.df[\"question\"][idx].split(\" \")\n",
    "        for word in question_words:\n",
    "            try:\n",
    "                question[self.question2idx[word]] = 1  # one-hot表現に変換\n",
    "            except KeyError:\n",
    "                question[-1] = 1  # 未知語\n",
    "        \n",
    "        question = [0, 1, 2, 3]\n",
    "\n",
    "        if self.answer:\n",
    "            answers = [self.answer2idx[process_text(answer[\"answer\"])] for answer in self.df[\"answers\"][idx]]\n",
    "            mode_answer_idx = mode(answers)  # 最頻値を取得（正解ラベル）\n",
    "\n",
    "            return image, torch.Tensor(question), torch.Tensor(answers), int(mode_answer_idx)\n",
    "\n",
    "        else:\n",
    "            return image, torch.Tensor(question)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 評価指標の実装\n",
    "# 簡単にするならBCEを利用する\n",
    "def VQA_criterion(batch_pred: torch.Tensor, batch_answers: torch.Tensor):\n",
    "    total_acc = 0.\n",
    "\n",
    "    for pred, answers in zip(batch_pred, batch_answers):\n",
    "        acc = 0.\n",
    "        for i in range(len(answers)):\n",
    "            num_match = 0\n",
    "            for j in range(len(answers)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if pred == answers[j]:\n",
    "                    num_match += 1\n",
    "            acc += min(num_match / 3, 1)\n",
    "        total_acc += acc / 10\n",
    "\n",
    "    return total_acc / len(batch_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patching(nn.Module):\n",
    "    def __init__(self, patch_size):\n",
    "        \"\"\" [input]\n",
    "            - patch_size (int) : パッチの縦の長さ（=横の長さ）\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = Rearrange(\"b c (h ph) (w pw) -> b (h w) (ph pw c)\", ph = patch_size, pw = patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" [input]\n",
    "            - x (torch.Tensor) : 画像データ\n",
    "                - x.shape = torch.Size([batch_size, channels, image_height, image_width])\n",
    "        \"\"\"\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProjection(nn.Module):\n",
    "    def __init__(self, patch_dim, dim):\n",
    "        \"\"\" [input]\n",
    "            - patch_dim (int) : 一枚あたりのパッチのベクトルの長さ（= channels * (patch_size ** 2)）\n",
    "            - dim (int) : パッチのベクトルが変換されたベクトルの長さ \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(patch_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" [input]\n",
    "            - x (torch.Tensor) \n",
    "                - x.shape = torch.Size([batch_size, n_patches, patch_dim])\n",
    "        \"\"\"\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbedding(nn.Module):\n",
    "    def __init__(self, dim, n_patches):\n",
    "        \"\"\" [input]\n",
    "            - dim (int) : パッチのベクトルが変換されたベクトルの長さ\n",
    "            - n_patches (int) : パッチの枚数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # [class] token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "\n",
    "        # position embedding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, n_patches + 1, dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"[input]\n",
    "            - x (torch.Tensor)\n",
    "                - x.shape = torch.Size([batch_size, n_patches, dim])\n",
    "        \"\"\"\n",
    "        # バッチサイズを抽出\n",
    "        batch_size, _, __ = x.shape\n",
    "\n",
    "        # [class] トークン付加\n",
    "        # x.shape : [batch_size, n_patches, patch_dim] -> [batch_size, n_patches + 1, patch_dim]\n",
    "        cls_tokens = repeat(self.cls_token, \"1 1 d -> b 1 d\", b = batch_size)\n",
    "        x = torch.concat([cls_tokens, x], dim = 1)\n",
    "\n",
    "        # 位置エンコーディング\n",
    "        x += self.pos_embedding\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
    "        \"\"\"\n",
    "        入力画像をパッチごとに埋め込むための層．\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        image_size : Tuple[int]\n",
    "            入力画像のサイズ．\n",
    "        patch_size : Tuple[int]\n",
    "            各パッチのサイズ．\n",
    "        in_channels : int\n",
    "            入力画像のチャネル数．\n",
    "        embed_dim : int\n",
    "            埋め込み後の次元数．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        image_height, image_width = image_size\n",
    "        patch_height, patch_width = patch_size\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, \"パッチサイズは，入力画像のサイズを割り切れる必要があります．\"\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)  # パッチの数\n",
    "        patch_dim = in_channels * patch_height * patch_width  # 各パッチを平坦化したときの次元数\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=patch_height, p2=patch_width),  # 画像をパッチに分割して平坦化\n",
    "            nn.Linear(in_features=patch_dim, out_features=embed_dim),  # 埋め込みを行う\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        B: バッチサイズ\n",
    "        C: 入力画像のチャネル数\n",
    "        H: 入力画像の高さ\n",
    "        W: 入力画像の幅\n",
    "        \"\"\"\n",
    "        return self.to_patch_embedding(x)  # (B, C, H, W) -> (B, num_patches, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://qiita.com/age884/items/bc27c532a6d7c720fc3e\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        dim : int\n",
    "            入力データの次元数．\n",
    "        hidden_dim : int\n",
    "            隠れ層の次元．\n",
    "        dropout : float\n",
    "            各全結合層の後のDropoutの確率(default=0.)．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=dim, out_features=hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        (B, D) -> (B, D)\n",
    "        B: バッチサイズ\n",
    "        D: 次元数\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, heads, dim_head, dropout=0.):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        dim : int\n",
    "            入力データの次元数．埋め込み次元数と一致する．\n",
    "        heads : int\n",
    "            ヘッドの数．\n",
    "        dim_head : int\n",
    "            各ヘッドのデータの次元数．\n",
    "        dropout : float\n",
    "            Dropoutの確率(default=0.)．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.dim_head = dim_head\n",
    "        inner_dim = dim_head * heads  # ヘッドに分割する前のQ, K, Vの次元数．self.dimと異なっても良い．\n",
    "        project_out = not (heads == 1 and dim_head == dim)  # headsが1，dim_headがdimと等しければ通常のSelf-Attention\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = math.sqrt(dim_head)  # ソフトマックス関数を適用する前のスケーリング係数(dim_k)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)  # アテンションスコアの算出に利用するソフトマックス関数\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Q, K, Vに変換するための全結合層\n",
    "        self.to_q = nn.Linear(in_features=dim, out_features=inner_dim)\n",
    "        self.to_k = nn.Linear(in_features=dim, out_features=inner_dim)\n",
    "        self.to_v = nn.Linear(in_features=dim, out_features=inner_dim)\n",
    "\n",
    "        # dim != inner_dimなら線形層を入れる，そうでなければそのまま出力\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(in_features=inner_dim, out_features=dim),\n",
    "            nn.Dropout(dropout),\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        B: バッチサイズ\n",
    "        N: 系列長\n",
    "        D: データの次元数(dim)\n",
    "        \"\"\"\n",
    "        B, N, D = x.size()\n",
    "\n",
    "        # 入力データをQ, K, Vに変換する\n",
    "        # (B, N, dim) -> (B, N, inner_dim)\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(x)\n",
    "\n",
    "        # Q, K, Vをヘッドに分割する\n",
    "        # (B, N, inner_dim) -> (B, heads, N, dim_head)\n",
    "        q = rearrange(q, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n",
    "        k = rearrange(k, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n",
    "        v = rearrange(v, \"b n (h d) -> b h n d\", h=self.heads, d=self.dim_head)\n",
    "\n",
    "        # QK^T / sqrt(d_k)を計算する\n",
    "        # (B, heads, N, dim_head) x (B, heads, dim_head, N) -> (B, heads, N, N)\n",
    "        dots = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        # ソフトマックス関数でスコアを算出し，Dropoutをする\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # softmax(QK^T / sqrt(d_k))Vを計算する\n",
    "        # (B, heads, N, N) x (B, heads, N, dim_head) -> (B, heads, N, dim_head)\n",
    "        out = torch.matmul(attn ,v)\n",
    "\n",
    "        # もとの形に戻す\n",
    "        # (B, heads, N, dim_head) -> (B, N, dim)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\", h=self.heads, d=self.dim_head)\n",
    "\n",
    "        # 次元が違っていればもとに戻して出力\n",
    "        # 表現の可視化のためにattention mapも返すようにしておく\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, dim, heads, dim_head, mlp_dim, dropout):\n",
    "        \"\"\"\n",
    "        TransformerのEncoder Blockの実装．\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        dim : int\n",
    "            埋め込みされた次元数．PatchEmbedのembed_dimと同じ値．\n",
    "        heads : int\n",
    "            Multi-Head Attentionのヘッドの数．\n",
    "        dim_head : int\n",
    "            Multi-Head Attentionの各ヘッドの次元数．\n",
    "        mlp_dim : int\n",
    "            Feed-Forward Networkの隠れ層の次元数．\n",
    "        dropout : float\n",
    "            Droptou層の確率p．\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn_ln = nn.LayerNorm(dim)  # Attention前のLayerNorm\n",
    "        self.attn = MultiHeadAttention(dim, heads, dim_head, dropout)\n",
    "        self.ffn_ln = nn.LayerNorm(dim)  # FFN前のLayerNorm\n",
    "        self.ffn = MLP(dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "        \"\"\"\n",
    "        x: (B, N, dim)\n",
    "        B: バッチサイズ\n",
    "        N: 系列長\n",
    "        dim: 埋め込み次元\n",
    "        \"\"\"\n",
    "        y = self.attn(self.attn_ln(x))\n",
    "        x = y + x\n",
    "        out = self.ffn(self.ffn_ln(x)) + x\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, vocab_size, n_classes, dim, depth, n_heads, dropout = 0., channels = 3, mlp_dim = 256):\n",
    "        \"\"\" [input]\n",
    "            - image_size (int) : 画像の縦の長さ（= 横の長さ）\n",
    "            - patch_size (int) : パッチの縦の長さ（= 横の長さ）\n",
    "            - n_classes (int) : 分類するクラスの数\n",
    "            - dim (int) : 各パッチのベクトルが変換されたベクトルの長さ（参考[1] (1)式 D）\n",
    "            - depth (int) : Transformer Encoder の層の深さ（参考[1] (2)式 L）\n",
    "            - n_heads (int) : Multi-Head Attention の head の数\n",
    "            - chahnnels (int) : 入力のチャネル数（RGBの画像なら3）\n",
    "            - mlp_dim (int) : MLP の隠れ層のノード数\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        # Params\n",
    "        n_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = channels * patch_size * patch_size\n",
    "        self.depth = depth\n",
    "        self.dim = dim\n",
    "\n",
    "        # Layers\n",
    "        self.patching = Patching(patch_size = patch_size)\n",
    "        self.linear_projection_of_flattened_patches = LinearProjection(patch_dim = patch_dim, dim = dim)\n",
    "        self.embedding = ImageEmbedding(dim = dim, n_patches = n_patches)\n",
    "        self.transformer_encoder = torch.nn.Sequential(*[TransformerEncoder(dim=dim, heads=n_heads, dim_head=dim, mlp_dim=mlp_dim, dropout=dropout) for _ in range(depth)])\n",
    "        self.mlp_head = MLPHead(dim = dim, out_dim = n_classes)\n",
    "\n",
    "        self.cls_token = torch.nn.Parameter(torch.randn(1, 1, dim))  # class tokenの初期化\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.randn(1, n_patches, dim))  # positional embedding（学習可能にしている）\n",
    "        \n",
    "        # 入力画像をパッチに分割する\n",
    "        self.patchify = PatchEmbedding([image_size, image_size], [patch_size, patch_size], 3, dim)\n",
    "\n",
    "        # textr\n",
    "        self.text_embedding = nn.Embedding(vocab_size, dim, padding_idx = 0)\n",
    "        self.pos_encoder = PositionalEncoding(dim, dropout)\n",
    "\n",
    "\n",
    "    def forward(self, image, question):\n",
    "        \"\"\" [input]\n",
    "            - img (torch.Tensor) : 画像データ\n",
    "                - img.shape = torch.Size([batch_size, channels, image_height, image_width])\n",
    "        \"\"\"\n",
    "\n",
    "        x_img = image\n",
    "        x_qst = question\n",
    "\n",
    "        print(x_qst.shape)\n",
    "        # print(x_qst)\n",
    "\n",
    "        # 1. 画像をパッチに分割\n",
    "        # x_img.shape : [batch_size, channels, image_height, image_width] -> [batch_size, n_patches, channels * (patch_size ** 2)]\n",
    "        # x_img = self.patching(x_img)\n",
    "\n",
    "        # 1. 入力画像をパッチに分割して，positional embeddingする\n",
    "        x_img = self.patchify(x_img)\n",
    "        x_img = x_img + self.pos_embedding\n",
    "\n",
    "        print(x_img.shape)\n",
    "\n",
    "        # 2. 各パッチをベクトルに変換\n",
    "        # x.shape : [batch_size, n_patches, channels * (patch_size ** 2)] -> [batch_size, n_patches, dim]\n",
    "        # x_img = self.linear_projection_of_flattened_patches(x_img)\n",
    "\n",
    "        # print(x_img.shape)\n",
    "\n",
    "        # 3. [class] トークン付加 + 位置エンコーディング \n",
    "        # x.shape : [batch_size, n_patches, dim] -> [batch_size, n_patches + 1, dim]\n",
    "        x_img = self.embedding(x_img)\n",
    "\n",
    "        print(x_img.shape)\n",
    "\n",
    "        # 4. Text Embedding\n",
    "        # x_qst.shape : [batch_size, text len] -> [batch_size, text len, dim]\n",
    "        x_qst = self.text_embedding(x_qst) * math.sqrt(self.dim)\n",
    "        x_qst = self.pos_encoder(x_qst)\n",
    "\n",
    "        print(x_qst.shape)\n",
    "        print(\"3\")\n",
    "\n",
    "        # Concat feature\n",
    "        x = torch.cat([x_img, x_qst], dim=1)\n",
    "\n",
    "        print(x.shape)\n",
    "        print(\"4\")\n",
    "\n",
    "        # 4. Transformer Encoder\n",
    "        # x.shape : No Change\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        print(x.shape)\n",
    "\n",
    "        # 5. 出力の0番目のベクトルを MLP Head で処理\n",
    "        # x.shape : [batch_size, n_patches + 1, dim] -> [batch_size, dim] -> [batch_size, n_classes]\n",
    "        x = x[:, 0]\n",
    "        x = self.mlp_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    image_size=222,\n",
    "    patch_size=74,\n",
    "    vocab_size=526,\n",
    "    n_classes=10,\n",
    "    dim=256,\n",
    "    depth=3,\n",
    "    dropout=0.1,\n",
    "    n_heads=4,\n",
    "    mlp_dim = 256\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 学習の実装\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    simple_acc = 0\n",
    "\n",
    "    start = time.time()\n",
    "    for image, question, answers, mode_answer in dataloader:\n",
    "        question = torch.tensor([[0, 1, 2, 3] for _ in range(128)])\n",
    "        image, question, answers, mode_answer = \\\n",
    "            image.to(device), question.to(device), answers.to(device), mode_answer.to(device)\n",
    "\n",
    "        pred = model(image, question)\n",
    "        \n",
    "        print(pred)\n",
    "\n",
    "        loss = criterion(pred, mode_answer.squeeze())\n",
    "\n",
    "        print(loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\n",
    "        simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()  # simple accuracy\n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloader, optimizer, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    simple_acc = 0\n",
    "\n",
    "    start = time.time()\n",
    "    for image, question, answers, mode_answer in dataloader:\n",
    "        image, question, answers, mode_answer = \\\n",
    "            image.to(device), question.to(device), answers.to(device), mode_answer.to(device)\n",
    "\n",
    "        pred = model(image, question)\n",
    "        loss = criterion(pred, mode_answer.squeeze())\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\n",
    "        simple_acc += (pred.argmax(1) == mode_answer).mean().item()  # simple accuracy\n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "# dataloader / model\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((222, 222)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "train_dataset = VQADataset(df_path=\"./data/train.json\", image_dir=\"./data/train\", transform=transform)\n",
    "test_dataset = VQADataset(df_path=\"./data/valid.json\", image_dir=\"./data/valid\", transform=transform, answer=False)\n",
    "test_dataset.update_dict(train_dataset)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer / criterion\n",
    "num_epoch = 20\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# criterionをKL-Divergenceに変更\n",
    "# criterion = nn.KLDivLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 4])\n",
      "torch.Size([128, 9, 256])\n",
      "torch.Size([128, 10, 256])\n",
      "torch.Size([128, 4, 256])\n",
      "3\n",
      "torch.Size([128, 14, 256])\n",
      "4\n",
      "torch.Size([128, 14, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epoch):\n\u001b[1;32m----> 3\u001b[0m     train_loss, train_acc, train_simple_acc, train_time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m【\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m】\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [s]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain simple acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_simple_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     15\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(image, question)\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, mode_answer\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\ATSUHIRO\\anaconda3\\envs\\dl_competition\\lib\\site-packages\\torch\\_tensor.py:464\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    461\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[0;32m    462\u001b[0m     )\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ATSUHIRO\\anaconda3\\envs\\dl_competition\\lib\\site-packages\\torch\\_tensor_str.py:697\u001b[0m, in \u001b[0;36m_str\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[0;32m    696\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[1;32m--> 697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ATSUHIRO\\anaconda3\\envs\\dl_competition\\lib\\site-packages\\torch\\_tensor_str.py:617\u001b[0m, in \u001b[0;36m_str_intern\u001b[1;34m(inp, tensor_contents)\u001b[0m\n\u001b[0;32m    615\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[0;32m    616\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 617\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[0;32m    620\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[1;32mc:\\Users\\ATSUHIRO\\anaconda3\\envs\\dl_competition\\lib\\site-packages\\torch\\_tensor_str.py:349\u001b[0m, in \u001b[0;36m_tensor_str\u001b[1;34m(self, indent)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[0;32m    347\u001b[0m     )\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 349\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[1;32mc:\\Users\\ATSUHIRO\\anaconda3\\envs\\dl_competition\\lib\\site-packages\\torch\\_tensor_str.py:138\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_select(\n\u001b[1;32m--> 138\u001b[0m         tensor_view, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m tensor_view\u001b[38;5;241m.\u001b[39mne(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "for epoch in range(num_epoch):\n",
    "    train_loss, train_acc, train_simple_acc, train_time = train(model, train_loader, optimizer, criterion, device)\n",
    "    print(f\"【{epoch + 1}/{num_epoch}】\\n\"\n",
    "            f\"train time: {train_time:.2f} [s]\\n\"\n",
    "            f\"train loss: {train_loss:.4f}\\n\"\n",
    "            f\"train acc: {train_acc:.4f}\\n\"\n",
    "            f\"train simple acc: {train_simple_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出用ファイルの作成\n",
    "model.eval()\n",
    "submission = []\n",
    "for image, question in test_loader:\n",
    "    image, question = image.to(device), question.to(device)\n",
    "    pred = model(image, question)\n",
    "    pred = pred.argmax(1).cpu().item()\n",
    "    submission.append(pred)\n",
    "\n",
    "submission = [train_dataset.idx2answer[id] for id in submission]\n",
    "submission = np.array(submission)\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "np.save(\"submission.npy\", submission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_competition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
