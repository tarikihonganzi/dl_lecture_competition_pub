{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "from statistics import mode\n",
    "import json \n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import rearrange\n",
    "from einops import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_adv(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 数詞を数字に変換\n",
    "    num_word_to_digit = {\n",
    "        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n",
    "        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n",
    "        'ten': '10', \n",
    "    }\n",
    "    for word, digit in num_word_to_digit.items():\n",
    "        text = text.replace(word, digit)\n",
    "\n",
    "    # 小数点のピリオドを削除\n",
    "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n",
    "\n",
    "    # 冠詞の削除\n",
    "    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n",
    "\n",
    "    # 短縮形のカンマの追加\n",
    "    contractions = {\n",
    "        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\n",
    "        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\", \"whats\": \"what's\",\n",
    "        \"thats\": \"that's\", \"whos\": \"who's\", \"wheres\": \"where's\", \"whens\": \"when's\",\n",
    "        \"please\":\"\", \"could you\": \"can you\", \"could i\": \"can i\", \"could we\": \"can we\",\n",
    "    }\n",
    "\n",
    "    contractions_2 = {\n",
    "        \"theatre\" : \"theater\", \"colour\" : \"color\", \"centre\" : \"center\", \"favourite\" : \"favorite\",\n",
    "        \"travelling\" : \"traveling\", \"counselling\" : \"counseling\", \"metre\" : \"meter\",\n",
    "        \"cancelled\" : \"canceled\", \"labour\" : \"labor\", \"organisation\" : \"organization\",\n",
    "        \"calibre\" : \"caliber\", \"cheque\" : \"check\", \"manoeuvre\" : \"maneuver\",\n",
    "        \"neighbour\" : \"neighbor\", \"grey\" : \"gray\", \"dialogue\" : \"dialog\",\n",
    "    }\n",
    "\n",
    "    # contractions_3 = {\n",
    "    #     \"what is\": \"what's\", \"who is\": \"who's\", \"where is\": \"where's\", \"when is\": \"when's\",\n",
    "    #     \"how is\": \"how's\", \"it is\": \"it's\", \"he is\": \"he's\", \"she is\": \"she's\",\n",
    "    #     \"that is\": \"that's\", \"there is\": \"there's\", \"here is\": \"here's\",\n",
    "    #     \"i am\": \"i'm\", \"you are\": \"you're\", \"we are\": \"we're\", \"they are\": \"they're\",\n",
    "    #     \"i have\": \"i've\", \"you have\": \"you've\", \"we have\": \"we've\", \"they have\": \"they've\",\n",
    "    #     \"i will\": \"i'll\", \"you will\": \"you'll\",\n",
    "    # }\n",
    "    contractions_3 = {\n",
    "        \"what's\" : \"what is\", \"who's\" : \"who is\", \"where's\" : \"where is\", \"when's\" : \"when is\",\n",
    "        \"how's\" : \"how is\", \"it's\" : \"it is\", \"he's\" : \"he is\", \"she's\" : \"she is\",\n",
    "        \"that's\" : \"that is\", \"there's\" : \"there is\", \"here's\" : \"here is\",\n",
    "        \"i'm\" : \"i am\", \"you're\" : \"you are\", \"we're\" : \"we are\", \"they're\" : \"they are\",\n",
    "        \"i've\" : \"i have\", \"you've\" : \"you have\", \"we've\" : \"we have\", \"they've\" : \"they have\",\n",
    "        \"i'll\" : \"i will\", \"you'll\" : \"you will\",\n",
    "    }\n",
    " \n",
    "    for contraction, correct in contractions.items():\n",
    "        text = text.replace(contraction, correct)\n",
    "    for contraction, correct in contractions_2.items():\n",
    "        text = text.replace(contraction, correct)\n",
    "    for contraction, correct in contractions_3.items():\n",
    "        text = text.replace(contraction, correct)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "\n",
    "    # 連続するスペースを1つに変換\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_adv(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 数詞を数字に変換\n",
    "    num_word_to_digit = {\n",
    "        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n",
    "        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n",
    "        'ten': '10', \n",
    "    }\n",
    "    for word, digit in num_word_to_digit.items():\n",
    "        text = text.replace(word, digit)\n",
    "\n",
    "    # 小数点のピリオドを削除\n",
    "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n",
    "\n",
    "    # 冠詞の削除\n",
    "    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n",
    "\n",
    "    # 短縮形のカンマの追加\n",
    "    contractions = {\n",
    "        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\n",
    "        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\", \"whats\": \"what's\",\n",
    "        \"thats\": \"that's\", \"whos\": \"who's\", \"wheres\": \"where's\", \"whens\": \"when's\",\n",
    "        \"please\":\"\", \"could you\": \"can you\", \"could i\": \"can i\", \"could we\": \"can we\",\n",
    "    }\n",
    "\n",
    "    contractions_2 = {\n",
    "        \"theatre\" : \"theater\", \"colour\" : \"color\", \"centre\" : \"center\", \"favourite\" : \"favorite\",\n",
    "        \"travelling\" : \"traveling\", \"counselling\" : \"counseling\", \"metre\" : \"meter\",\n",
    "        \"cancelled\" : \"canceled\", \"labour\" : \"labor\", \"organisation\" : \"organization\",\n",
    "        \"calibre\" : \"caliber\", \"cheque\" : \"check\", \"manoeuvre\" : \"maneuver\",\n",
    "        \"neighbour\" : \"neighbor\", \"grey\" : \"gray\", \"dialogue\" : \"dialog\",\n",
    "    }\n",
    "\n",
    "    # contractions_3 = {\n",
    "    #     \"what is\": \"what's\", \"who is\": \"who's\", \"where is\": \"where's\", \"when is\": \"when's\",\n",
    "    #     \"how is\": \"how's\", \"it is\": \"it's\", \"he is\": \"he's\", \"she is\": \"she's\",\n",
    "    #     \"that is\": \"that's\", \"there is\": \"there's\", \"here is\": \"here's\",\n",
    "    #     \"i am\": \"i'm\", \"you are\": \"you're\", \"we are\": \"we're\", \"they are\": \"they're\",\n",
    "    #     \"i have\": \"i've\", \"you have\": \"you've\", \"we have\": \"we've\", \"they have\": \"they've\",\n",
    "    #     \"i will\": \"i'll\", \"you will\": \"you'll\",\n",
    "    # }\n",
    "    contractions_3 = {\n",
    "        \"what's\" : \"what is\", \"who's\" : \"who is\", \"where's\" : \"where is\", \"when's\" : \"when is\",\n",
    "        \"how's\" : \"how is\", \"it's\" : \"it is\", \"he's\" : \"he is\", \"she's\" : \"she is\",\n",
    "        \"that's\" : \"that is\", \"there's\" : \"there is\", \"here's\" : \"here is\",\n",
    "        \"i'm\" : \"i am\", \"you're\" : \"you are\", \"we're\" : \"we are\", \"they're\" : \"they are\",\n",
    "        \"i've\" : \"i have\", \"you've\" : \"you have\", \"we've\" : \"we have\", \"they've\" : \"they have\",\n",
    "        \"i'll\" : \"i will\", \"you'll\" : \"you will\",\n",
    "    }\n",
    " \n",
    "    for contraction, correct in contractions.items():\n",
    "        text = text.replace(contraction, correct)\n",
    "    for contraction, correct in contractions_2.items():\n",
    "        text = text.replace(contraction, correct)\n",
    "    for contraction, correct in contractions_3.items():\n",
    "        text = text.replace(contraction, correct)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "\n",
    "    # 連続するスペースを1つに変換\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 数詞を数字に変換\n",
    "    num_word_to_digit = {\n",
    "        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n",
    "        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n",
    "        'ten': '10'\n",
    "    }\n",
    "    for word, digit in num_word_to_digit.items():\n",
    "        text = text.replace(word, digit)\n",
    "\n",
    "    # 小数点のピリオドを削除\n",
    "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n",
    "\n",
    "    # 冠詞の削除\n",
    "    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n",
    "\n",
    "    # 短縮形のカンマの追加\n",
    "    contractions = {\n",
    "        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\n",
    "        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\"\n",
    "    }\n",
    "    for contraction, correct in contractions.items():\n",
    "        text = text.replace(contraction, correct)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "\n",
    "    # 連続するスペースを1つに変換\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_path, image_dir, transform=None, answer=True):\n",
    "        self.transform = transform  # 画像の前処理\n",
    "        self.image_dir = image_dir  # 画像ファイルのディレクトリ\n",
    "        self.df = pandas.read_json(df_path)  # 画像ファイルのパス，question, answerを持つDataFrame\n",
    "        self.answer = answer\n",
    "\n",
    "        # question / answerの辞書を作成\n",
    "        self.question2idx = {}\n",
    "        self.answer2idx = {}\n",
    "        self.idx2question = {}\n",
    "        self.idx2answer = {}\n",
    "        self.questionInitialWords = dict()\n",
    "        c1 = c2 = c3 = c4 = c5 = c6 = c7 = 0\n",
    "\n",
    "        # 質問文に含まれる単語を辞書に追加\n",
    "        for question in self.df[\"question\"]:\n",
    "            question = process_text_adv(question)\n",
    "            words = question.split(\" \")\n",
    "            \n",
    "            for idex, word in enumerate(words):\n",
    "                if idex == 0:\n",
    "                    if word not in self.questionInitialWords:\n",
    "                        self.questionInitialWords[word] = 1\n",
    "                    else:\n",
    "                        self.questionInitialWords[word] += 1            \n",
    "                if word not in self.question2idx:\n",
    "                    self.question2idx[word] = len(self.question2idx)\n",
    "        self.idx2question = {v: k for k, v in self.question2idx.items()}  # 逆変換用の辞書(question)\n",
    "        print(self.questionInitialWords)\n",
    "        print(len(self.questionInitialWords))\n",
    "\n",
    "        if self.answer:\n",
    "            # 回答に含まれる単語を辞書に追加\n",
    "            for answers in self.df[\"answers\"]:\n",
    "                for answer in answers:\n",
    "                    word = answer[\"answer\"]\n",
    "                    word = process_text(word)\n",
    "                    if word not in self.answer2idx:\n",
    "                        self.answer2idx[word] = len(self.answer2idx)\n",
    "            self.idx2answer = {v: k for k, v in self.answer2idx.items()}  # 逆変換用の辞書(answer)\n",
    "\n",
    "            # print(f\"answer{self.idx2answer}\")\n",
    "\n",
    "    def update_dict(self, dataset):\n",
    "        \"\"\"\n",
    "        検証用データ，テストデータの辞書を訓練データの辞書に更新する．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : Dataset\n",
    "            訓練データのDataset\n",
    "        \"\"\"\n",
    "        self.question2idx = dataset.question2idx\n",
    "        self.answer2idx = dataset.answer2idx\n",
    "        self.idx2question = dataset.idx2question\n",
    "        self.idx2answer = dataset.idx2answer\n",
    "\n",
    "    def __getitem__(self, idx, isKL=False):\n",
    "        \"\"\"\n",
    "        対応するidxのデータ（画像，質問，回答）を取得．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            取得するデータのインデックス\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        image : torch.Tensor  (C, H, W)\n",
    "            画像データ\n",
    "        question : torch.Tensor  (vocab_size)\n",
    "            質問文をone-hot表現に変換したもの\n",
    "        answers : torch.Tensor  (n_answer)\n",
    "            10人の回答者の回答のid\n",
    "        mode_answer_idx : torch.Tensor  (1)\n",
    "            10人の回答者の回答の中で最頻値の回答のid\n",
    "        \"\"\"\n",
    "        image = Image.open(f\"{self.image_dir}/{self.df['image'][idx]}\")\n",
    "        image = self.transform(image)\n",
    "        question = np.zeros(len(self.idx2question) + 1)  # 未知語用の要素を追加\n",
    "        question_words = self.df[\"question\"][idx].split(\" \")\n",
    "\n",
    "        for word in question_words:\n",
    "            try:\n",
    "                question[self.question2idx[word]] = 1  # one-hot表現に変換\n",
    "            except KeyError:\n",
    "                question[-1] = 1  # 未知語\n",
    "\n",
    "        if self.answer:\n",
    "            answers_prevector = np.zeros(len(self.answer2idx))\n",
    "            answers = [self.answer2idx[process_text(answer[\"answer\"])] for answer in self.df[\"answers\"][idx]]\n",
    "            for answer in answers:\n",
    "                answers_prevector[answer] += 1\n",
    "            sum_answers = max(np.sum(answers_prevector), 1e-10)\n",
    "            answers_vector = answers_prevector / sum_answers\n",
    "            mode_answer_idx = mode(answers)  # 最頻値を取得（正解ラベル）\n",
    "\n",
    "            return image, torch.Tensor(question), torch.Tensor(answers), int(mode_answer_idx), torch.Tensor(answers_vector)\n",
    "\n",
    "        else:\n",
    "            return image, torch.Tensor(question)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"answers\":{\"0\":[{\"answer_confidence\":\"yes\",\"answer\":\"beef chuck steak\"},{\"answer_confidence\":\"yes\",\"answer\":\"beef chuck steak\"},{\"answer_confidence\":\"yes\",\"answer\":\"beef chuck steak\"},{\"answer_confidence\":\"yes\",\"answer\":\"beef chuck steak\"},{\"answer_confidence\":\"yes\",\"answer\":\"flat iron beef chuck steak\"},{\"answer_confidence\":\"yes\",\"answer\":\"beef chuck steak\"},{\"answer_confidence\":\"yes\",\"answer\":\"steak\"},{\"answer_confidence\":\"yes\",\"answer\":\"flat iron beef chuck steak\"},{\"answer_confidence\":\"yes\",\"answer\":\"beef chuck steak\"},{\"answer_confidence\":\"yes\",\"answer\":\"beef chuck steak\"}],\"1\":[{\"answer_confidence\":\"yes\",\"answer\":\"unanswerable\"},{\"answer_confidence\":\"yes\",\"answer\":\"unanswerable\"},{\"answer_confidence\":\"yes\",\"answer\":\"unanswerable\"},{\"answer_confidence\":\"yes\",\"answer\":\"candle\"},{\"answer_confidence\":\"no\",\"answer\":\"unanswerable\"},{\"answer_confidence\":\"maybe\",\"answer\":\"unanswerable\"},{\"answer_confidence\":\"yes\",\"answer\":\"unanswerable\"},{\"answer_confidence\":\"yes\",\"answer\":\"unanswerable\"},{\"answer_confidence\":\"no\",\"answer\":\"unanswerable\"},{\"answer_confidence\":\"yes\",\"answer\":\"unanswerable\"}],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# COCO_val2014_000000393075\n",
    "\n",
    "answer_type = dict()\n",
    "extra_answer_output = dict()\n",
    "\n",
    "print(\"test\")\n",
    "# データセットの読み込み\n",
    "with open('./extra_data/train_question.json', 'r') as f:\n",
    "    question_data = json.load(f)\n",
    "\n",
    "with open('./extra_data/train_answer.json', 'r') as f:\n",
    "    answer_data = json.load(f)\n",
    "\n",
    "extra_questions_data = question_data[\"questions\"]\n",
    "extra_answer_data = answer_data[\"annotations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "214354\n",
      "{'image_id': 262148, 'question': 'Where is he looking?', 'question_id': 262148000}\n",
      "{'question_type': 'none of the above', 'multiple_choice_answer': 'down', 'answers': [{'answer': 'down', 'answer_confidence': 'yes', 'answer_id': 1}, {'answer': 'down', 'answer_confidence': 'yes', 'answer_id': 2}, {'answer': 'at table', 'answer_confidence': 'yes', 'answer_id': 3}, {'answer': 'skateboard', 'answer_confidence': 'yes', 'answer_id': 4}, {'answer': 'down', 'answer_confidence': 'yes', 'answer_id': 5}, {'answer': 'table', 'answer_confidence': 'yes', 'answer_id': 6}, {'answer': 'down', 'answer_confidence': 'yes', 'answer_id': 7}, {'answer': 'down', 'answer_confidence': 'yes', 'answer_id': 8}, {'answer': 'down', 'answer_confidence': 'yes', 'answer_id': 9}, {'answer': 'down', 'answer_confidence': 'yes', 'answer_id': 10}], 'image_id': 262148, 'answer_type': 'other', 'question_id': 262148000}\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")\n",
    "print(len(extra_questions_data))\n",
    "print(extra_questions_data[0])\n",
    "\n",
    "start_number = 19873\n",
    "\n",
    "question_map_dict = dict()\n",
    "image_dict = dict()\n",
    "question_dict = dict()\n",
    "final_dict = dict()\n",
    "\n",
    "for data in extra_questions_data:\n",
    "    original_image_id = data[\"image_id\"]\n",
    "    question_id = data[\"question_id\"]\n",
    "    image_name = \"COCO_train2014_\" + str(original_image_id).zfill(12) + \".jpg\"\n",
    "    \n",
    "    question_map_dict[question_id] = start_number\n",
    "    image_dict[start_number] = image_name\n",
    "    re_question_id = start_number\n",
    "    \n",
    "    question = data[\"question\"]\n",
    "    question_dict[re_question_id] = question\n",
    "\n",
    "    start_number += 1\n",
    "\n",
    "\n",
    "final_dict[\"image\"] = image_dict\n",
    "final_dict[\"question\"] = question_dict\n",
    "print(extra_answer_data[0])\n",
    "\n",
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./extra_data/test_question.json', 'w') as f:\n",
    "#     json.dump(question_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./extra_data/test_image.json', 'w') as f:\n",
    "#     json.dump(image_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'answer_confidence': 'yes', 'answer': 'down'}, {'answer_confidence': 'yes', 'answer': 'down'}, {'answer_confidence': 'yes', 'answer': 'at table'}, {'answer_confidence': 'yes', 'answer': 'skateboard'}, {'answer_confidence': 'yes', 'answer': 'down'}, {'answer_confidence': 'yes', 'answer': 'table'}, {'answer_confidence': 'yes', 'answer': 'down'}, {'answer_confidence': 'yes', 'answer': 'down'}, {'answer_confidence': 'yes', 'answer': 'down'}, {'answer_confidence': 'yes', 'answer': 'down'}]\n",
      "min_image_id: 42\n"
     ]
    }
   ],
   "source": [
    "min_image_id = 999999999999999999\n",
    "\n",
    "for data in extra_answer_data:\n",
    "    question_id = data[\"question_id\"]\n",
    "\n",
    "    re_question_id = question_map_dict[question_id]\n",
    "\n",
    "    image_id = data[\"image_id\"]\n",
    "    if image_id < min_image_id:\n",
    "        min_image_id = image_id\n",
    "    answer_list = data['answers']\n",
    "    replace_answer_list = []\n",
    "    for answer in answer_list:\n",
    "        answer_data = dict()\n",
    "        answer_data[\"answer_confidence\"] = answer[\"answer_confidence\"]\n",
    "        answer_data[\"answer\"] = answer[\"answer\"]\n",
    "        answer = answer_data\n",
    "        replace_answer_list.append(answer_data)\n",
    "    extra_answer_output[re_question_id] = replace_answer_list\n",
    "\n",
    "final_dict[\"answers\"] = extra_answer_output\n",
    "\n",
    "print(f\"{extra_answer_output[19873]}\")\n",
    "print(f\"min_image_id: {min_image_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./extra_data/test.json', 'w') as f:\n",
    "#     json.dump(extra_answer_output, f, indent=2)\n",
    "\n",
    "# with open('./extra_data/final_valie.json', 'w') as f:\n",
    "#     json.dump(final_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_data(df_path, image_dir=\"\", transform=None, answer=False):\n",
    "\n",
    "    df = pandas.read_json(df_path)\n",
    "\n",
    "    # question / answerの辞書を作成\n",
    "    question2idx = {}\n",
    "    answer2idx = {}\n",
    "    questionInitialWords = dict()\n",
    "    c1 = c2 = c3 = c4 = c5 = c6 = c7 = 0\n",
    "\n",
    "    first_three_words = []\n",
    "    first_four_words = []\n",
    "\n",
    "    # 質問文に含まれる単語を辞書に追加\n",
    "    for question in df[\"question\"]:\n",
    "        question = process_text_adv(question)\n",
    "        words = question.split(\" \")\n",
    "\n",
    "        if len(words) > 2:\n",
    "            first_three_words.append(words[0] + \" \" + words[1] + \" \" + words[2])\n",
    "            if len(words) > 3:\n",
    "                first_four_words.append(words[0] + \" \" + words[1] + \" \" + words[2] + \" \" + words[3])\n",
    "            \n",
    "        for idex, word in enumerate(words):\n",
    "            if idex == 0:\n",
    "                if word not in questionInitialWords:\n",
    "                    questionInitialWords[word] = 1\n",
    "                else:\n",
    "                    questionInitialWords[word] += 1            \n",
    "            if word not in question2idx:\n",
    "                question2idx[word] = len(question2idx)\n",
    "    idx2question = {v: k for k, v in question2idx.items()}  # 逆変換用の辞書(question)\n",
    "\n",
    "    if answer:\n",
    "        # 回答に含まれる単語を辞書に追加\n",
    "        for answers in df[\"answers\"]:\n",
    "            for answer in answers:\n",
    "                word = answer[\"answer\"]\n",
    "                word = process_text(word)\n",
    "                if word not in answer2idx:\n",
    "                    answer2idx[word] = len(answer2idx)\n",
    "        idx2answer = {v: k for k, v in answer2idx.items()}  # 逆変換用の辞書(answer)\n",
    "\n",
    "    return first_three_words, first_four_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214354/214354 [00:37<00:00, 5677.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter: 75567\n",
      "counter four: 18647\n",
      "counter only three: 1770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "three_words_1, four_words_1 = original_data('./data/train.json')\n",
    "three_words_2, four_words_2 = original_data('./data/valid.json')\n",
    "\n",
    "counter = 0\n",
    "counter_four = 0\n",
    "counter_only_three = 0\n",
    "\n",
    "dest = len(question_dict)\n",
    "\n",
    "start_number = 19873\n",
    "\n",
    "selected_question = []\n",
    "\n",
    "for idx in tqdm(range(dest)):\n",
    "    index = start_number + idx\n",
    "\n",
    "    new_first_three_words = \"\"\n",
    "    question_state = question_dict[index]\n",
    "    question_state = process_text_adv(question_state)\n",
    "    \n",
    "    question_list = question_state.split(\" \")\n",
    "    if len(question_list) <= 2:\n",
    "        continue \n",
    "    \n",
    "    new_first_three_words = question_list[0] + \" \" + question_list[1] + \" \" + question_list[2]\n",
    "    new_first_four_words = \"\"\n",
    "    if len(question_list) > 3:\n",
    "        new_first_four_words = question_list[0] + \" \" + question_list[1] + \" \" + question_list[2] + \" \" + question_list[3]\n",
    "\n",
    "\n",
    "    if new_first_three_words in three_words_2:\n",
    "        counter += 1\n",
    "        if len(question_list) > 3:\n",
    "            if new_first_four_words in four_words_1:\n",
    "                counter_four += 1\n",
    "        else:\n",
    "            counter_only_three += 1\n",
    "        selected_question.append(index)\n",
    "        continue\n",
    "\n",
    "    if new_first_three_words in three_words_1:    \n",
    "        counter += 1\n",
    "        if len(question_list) > 3:\n",
    "            if new_first_four_words in four_words_2:\n",
    "                counter_four += 1\n",
    "        else:\n",
    "            counter_only_three += 1\n",
    "        selected_question.append(index)\n",
    "        continue\n",
    "\n",
    "print(f\"counter: {counter}\")\n",
    "print(f\"counter four: {counter_four}\")\n",
    "print(f\"counter only three: {counter_only_three}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'what': 13937, 'maybe': 2, 'can': 1410, 'is': 949, 'tell': 186, 'when': 64, 'my': 9, 'you': 18, 'how': 366, 'for': 48, 'do': 98, 'i': 320, 'alright': 22, 'kind': 9, 'hi': 192, 'guy': 4, 'in': 26, 'would': 17, 'this': 192, 'yes': 75, 'which': 181, 'dinner': 2, 'describe': 86, 'who': 84, 'fall': 1, 'will': 17, 'now': 12, 'okay': 75, 'shut': 1, 'dishwasher': 1, 'read': 54, 'if': 18, 'uncle': 1, 'specifically': 5, 'grab': 2, 'oh': 11, 'are': 106, 'hiis': 1, 'hello': 98, 'were': 1, \"let's\": 7, \"i'd\": 8, 'say': 4, 'test': 6, 'does': 154, 'where': 124, 'so': 16, 'just': 18, 'end': 1, 'and': 12, 'calorie': 1, 'first': 2, 'slaver': 1, 'good': 14, 'brand': 3, 'little': 1, 'ask': 1, 'as': 1, 'wonder': 1, 'it': 28, 'security': 1, 'flavor': 5, 'ok': 47, 'there': 21, 'trying': 8, 'thank': 17, 'we': 6, 'brahma': 1, 'sparks': 2, 'identify': 24, 'sending': 1, 'hey': 31, 'ty': 1, 'next': 1, 'yeah': 17, 'last': 4, 'they': 2, 'bus': 1, 'screen': 3, 'video': 1, 'well': 5, 'all': 5, 'starting': 2, 'may': 5, 'set': 1, 'green': 1, 'calculate': 1, 'getting': 1, 'find': 4, 'explain': 7, 'instantaneous': 1, 'at': 1, 'name': 12, 'was': 5, 'let': 3, 'any': 7, 'whoops': 1, 'color': 34, 'resell': 1, 'am': 7, 'title': 5, 'computer': 1, 'guess': 1, 'participating': 4, 'should': 3, 'why': 10, 'have': 3, 'cooking': 2, 'hoping': 2, 'see': 3, 'thanks': 3, 'again': 2, 'clouds': 3, 'record': 15, 'ceremony': 1, 'that': 3, 'looking': 5, 'atom': 1, 'takes': 1, '1': 7, 'says': 1, 'sorry': 10, 'testing': 3, 'bottle': 1, 'use': 1, 'come': 3, 'barcode': 1, 'solve': 1, 'with': 1, 'expiration': 12, 'settings': 2, 'give': 2, 'some': 3, 'top': 2, 'salad': 1, 'could': 8, 'of': 2, 'ramen': 1, 'on': 15, 'from': 10, 'label': 3, 'display': 1, 'need': 3, 'question': 3, 'must': 1, 'currently': 1, 'nice': 1, 'around': 1, 'way': 2, 'product': 5, 'right': 4, 'help': 4, 'did': 12, 'still': 3, 'whatwhat': 1, 'here': 9, 'rate': 1, 'object': 1, 'coffee': 3, 'translate': 2, 'gotta': 1, 'being': 1, 'simplify': 4, 'wanna': 1, 'kit': 1, 'hola': 1, 'type': 6, 'error': 1, 'cam': 1, 'tv': 3, '5': 1, 'cd': 1, 'not': 2, 'excuse': 6, 'quest': 1, 'about': 2, '3': 3, 'description': 4, 'fist': 1, \"haven't\": 1, 'looks': 1, 'number': 4, 'cold': 1, \"didn't\": 1, \"can't\": 1, 'y': 1, 'hope': 2, 'microwave': 1, 'caffeinated': 2, 'whatpage': 1, 'some1': 1, 'hell': 1, 'code': 1, 'container': 2, 'temperature': 2, 'make': 2, 'know': 2, 'answer': 2, 'mean': 1, 'he': 1, 'piece': 2, 'no': 5, 'dark': 1, 'door': 1, 'jim': 2, 'cathy': 1, 'setting': 1, 'pattern': 1, 'red': 2, 'speaker': 1, 'speaker:': 2, 'sky': 5, 'out': 1, 'based': 1, 'record:': 1, 'actually': 1, 'has': 6, 'draw': 1, 'its': 3, 'sizes': 1, 'turn': 1, 'happy': 1, 'isd': 1, 'infinte': 1, 'id': 1, 'jacket': 1, 'creamer': 1, 'awesome': 1, 'call': 1, 'lets': 1, 'specials': 1, 'instructions': 1, 'keep': 1, 'hopefully': 9, 'michelle': 1, 'new': 1, 'per': 1, 'sell': 1, 'hard': 1, 'candle': 1, 'list': 1, 'checked': 1, 'cleaning': 1, 'stain': 1, 'send': 1, 'fully': 1, 'your': 1, 'take': 1, 'once': 1, 'vizwiz': 1, 'consuming': 1, 'since': 1, '9': 1, 'audio': 1, 'sometimes': 3, 'emily': 1, 'part': 1, 'de': 1, 'im': 2, 'limits': 1, 'chicken': 1, 'apple': 1, 'cup': 1, 'wine': 1, 'time': 1, 'small': 2, \"doesn't\": 1, 'summary': 1, 'perfume': 1, 'cookies': 1, 'true': 1, 'plant': 1, 'whar': 1, 'surface': 1, 'tea': 1, 'denomination': 1, 'dvd': 1, 'towel': 1, 'try': 2, 'disc': 1, 'package': 1, 'mine': 1, 'white': 1, 'above': 1, 'anything': 1, 'fire': 1, 'products': 1, 'que': 2, 'store': 1, 'desktop': 1, 'following': 2, 'shirt': 1, 'very': 1, 'alrighty': 1, 'go': 1, 'brandon': 1, 'detail': 1, 'tis': 1, 'confirm': 1, 'pop': 1, 'these': 2, 'letters': 1, 'garbled': 1, 'medication': 1, 'orca': 1, 'curious': 1, 'crock': 1, 'watch': 1, 'miss': 1, 'inaudible': 1, 'basically': 1, 'lights': 2, 'wondering': 1, 'shh': 1, 'by': 1, 'ring': 1, 'morning': 1, \"don't\": 1, 'ya': 1, 'able': 1, 'shape': 1, 'disk': 1, 'face': 1, 'cheese': 1, 'exactly': 2, 'second': 1, 'given': 1, 'creamy': 1, 'thermostat': 1, \"we'll\": 1, 'start': 1, 'same': 1, 'tube': 1, 'special': 1, 'ground': 1, 'hows': 1, 'kindly': 1, 'decaf': 1, 'si': 1, 'mrs': 1, 'franklin': 1, 'wat': 1, 'quarter': 1, 'paper': 1, 'wbat': 1, 'michael': 1}\n",
      "346\n"
     ]
    }
   ],
   "source": [
    "train_dataset = VQADataset(df_path=\"./data/train.json\", image_dir=\"./data/train\", transform=transform_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_competition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
