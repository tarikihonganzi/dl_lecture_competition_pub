{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "from statistics import mode\n",
    "\n",
    "from PIL import Image, ImageStat\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_adv(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 数詞を数字に変換\n",
    "    num_word_to_digit = {\n",
    "        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n",
    "        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n",
    "        'ten': '10', \n",
    "    }\n",
    "    for word, digit in num_word_to_digit.items():\n",
    "        text = text.replace(word, digit)\n",
    "\n",
    "    # 小数点のピリオドを削除\n",
    "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n",
    "\n",
    "    # 冠詞の削除\n",
    "    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n",
    "\n",
    "    # 短縮形のカンマの追加\n",
    "    contractions = {\n",
    "        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\n",
    "        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\", \"whats\": \"what's\",\n",
    "        \"thats\": \"that's\", \"whos\": \"who's\", \"wheres\": \"where's\", \"whens\": \"when's\",\n",
    "        \"please\":\"\", \"could you\": \"can you\", \"could i\": \"can i\", \"could we\": \"can we\",\n",
    "    }\n",
    "\n",
    "    contractions_2 = {\n",
    "        \"theatre\" : \"theater\", \"colour\" : \"color\", \"centre\" : \"center\", \"favourite\" : \"favorite\",\n",
    "        \"travelling\" : \"traveling\", \"counselling\" : \"counseling\", \"metre\" : \"meter\",\n",
    "        \"cancelled\" : \"canceled\", \"labour\" : \"labor\", \"organisation\" : \"organization\",\n",
    "        \"calibre\" : \"caliber\", \"cheque\" : \"check\", \"manoeuvre\" : \"maneuver\",\n",
    "        \"neighbour\" : \"neighbor\", \"grey\" : \"gray\", \"dialogue\" : \"dialog\",\n",
    "    }\n",
    "\n",
    "    contractions_3 = {\n",
    "        \"what is\": \"what's\", \"who is\": \"who's\", \"where is\": \"where's\", \"when is\": \"when's\",\n",
    "        \"how is\": \"how's\", \"it is\": \"it's\", \"he is\": \"he's\", \"she is\": \"she's\",\n",
    "        \"that is\": \"that's\", \"there is\": \"there's\", \"here is\": \"here's\",\n",
    "        \"i am\": \"i'm\", \"you are\": \"you're\", \"we are\": \"we're\", \"they are\": \"they're\",\n",
    "        \"i have\": \"i've\", \"you have\": \"you've\", \"we have\": \"we've\", \"they have\": \"they've\",\n",
    "        \"i will\": \"i'll\", \"you will\": \"you'll\",\n",
    "    }\n",
    "    # contractions_3 = {\n",
    "    #     \"what's\" : \"what is\", \"who's\" : \"who is\", \"where's\" : \"where is\", \"when's\" : \"when is\",\n",
    "    #     \"how's\" : \"how is\", \"it's\" : \"it is\", \"he's\" : \"he is\", \"she's\" : \"she is\",\n",
    "    #     \"that's\" : \"that is\", \"there's\" : \"there is\", \"here's\" : \"here is\",\n",
    "    #     \"i'm\" : \"i am\", \"you're\" : \"you are\", \"we're\" : \"we are\", \"they're\" : \"they are\",\n",
    "    #     \"i've\" : \"i have\", \"you've\" : \"you have\", \"we've\" : \"we have\", \"they've\" : \"they have\",\n",
    "    #     \"i'll\" : \"i will\", \"you'll\" : \"you will\",\n",
    "    # }\n",
    " \n",
    "    for contraction, correct in contractions.items():\n",
    "        text = text.replace(contraction, correct)\n",
    "    for contraction, correct in contractions_2.items():\n",
    "        text = text.replace(contraction, correct)\n",
    "    for contraction, correct in contractions_3.items():\n",
    "        text = text.replace(contraction, correct)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "\n",
    "    # 連続するスペースを1つに変換\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 数詞を数字に変換\n",
    "    num_word_to_digit = {\n",
    "        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n",
    "        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n",
    "        'ten': '10'\n",
    "    }\n",
    "    for word, digit in num_word_to_digit.items():\n",
    "        text = text.replace(word, digit)\n",
    "\n",
    "    # 小数点のピリオドを削除\n",
    "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n",
    "\n",
    "    # 冠詞の削除\n",
    "    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n",
    "\n",
    "    # 短縮形のカンマの追加\n",
    "    contractions = {\n",
    "        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\n",
    "        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\"\n",
    "    }\n",
    "    for contraction, correct in contractions.items():\n",
    "        text = text.replace(contraction, correct)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "\n",
    "    # 連続するスペースを1つに変換\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. データローダーの作成\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_path, image_dir, transform=None, transform_adv=None, answer=True):\n",
    "        self.transform = transform  # 画像の前処理\n",
    "        self.transform_adv = transform_adv  # 画像の前処理\n",
    "        self.image_dir = image_dir  # 画像ファイルのディレクトリ\n",
    "        self.df = pandas.read_json(df_path)  # 画像ファイルのパス，question, answerを持つDataFrame\n",
    "        print(f\"df shape: {self.df.shape}\")\n",
    "        \n",
    "        self.answer = answer\n",
    "\n",
    "        # question / answerの辞書を作成\n",
    "        self.question2idx = {}\n",
    "        self.answer2idx = {}\n",
    "        self.idx2question = {}\n",
    "        self.idx2answer = {}\n",
    "        c1 = c2 = c3 = c4 = c5 = c6 = c7 = 0\n",
    "\n",
    "        # 質問文の最大長を取得\n",
    "        self.max_question_length = 0\n",
    "\n",
    "        # 質問文に含まれる単語を辞書に追加\n",
    "        for question in self.df[\"question\"]:\n",
    "            question = process_text_adv(question)\n",
    "            words = question.split(\" \")\n",
    "\n",
    "            if len(words) > self.max_question_length:\n",
    "                self.max_question_length = len(words)\n",
    "            \n",
    "            for idex, word in enumerate(words):            \n",
    "                if word not in self.question2idx:\n",
    "                    self.question2idx[word] = len(self.question2idx)\n",
    "\n",
    "        \n",
    "        self.idx2question = {v: k for k, v in self.question2idx.items()}  # 逆変換用の辞書(question)\n",
    "\n",
    "        self.answer2idx[\"<unk>\"] = 0\n",
    "\n",
    "        if self.answer:\n",
    "            # 回答に含まれる単語を辞書に追加\n",
    "            for answers in self.df[\"answers\"]:\n",
    "                for answer in answers:\n",
    "                    word = answer[\"answer\"]\n",
    "                    word = process_text(word)\n",
    "                    if word not in self.answer2idx:\n",
    "                        self.answer2idx[word] = len(self.answer2idx)\n",
    "            \n",
    "            self.idx2answer = {v: k for k, v in self.answer2idx.items()}  # 逆変換用の辞書(answer)\n",
    "\n",
    "            # print(f\"answer{self.idx2answer}\")\n",
    "\n",
    "    def update_dict(self, dataset):\n",
    "        \"\"\"\n",
    "        検証用データ，テストデータの辞書を訓練データの辞書に更新する．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : Dataset\n",
    "            訓練データのDataset\n",
    "        \"\"\"\n",
    "        self.question2idx = dataset.question2idx\n",
    "        self.answer2idx = dataset.answer2idx\n",
    "        self.idx2question = dataset.idx2question\n",
    "        self.idx2answer = dataset.idx2answer\n",
    "\n",
    "    def __getitem__(self, idx, isKL=False):\n",
    "        \"\"\"\n",
    "        対応するidxのデータ（画像，質問，回答）を取得．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            取得するデータのインデックス\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        image : torch.Tensor  (C, H, W)\n",
    "            画像データ\n",
    "        question : torch.Tensor  (vocab_size)\n",
    "            質問文をone-hot表現に変換したもの\n",
    "        answers : torch.Tensor  (n_answer)\n",
    "            10人の回答者の回答のid\n",
    "        mode_answer_idx : torch.Tensor  (1)\n",
    "            10人の回答者の回答の中で最頻値の回答のid\n",
    "        \"\"\"\n",
    "        image = Image.open(f\"{self.image_dir}/{self.df['image'][idx]}\").convert(\"RGB\")\n",
    "        \n",
    "        # image = self.transform(image)\n",
    "        question = np.zeros(len(self.idx2question) + 1)  # 未知語用の要素を追加\n",
    "        question_vector = []\n",
    "        # question_words = self.df[\"question\"][idx].split(\" \")\n",
    "        question_text = process_text_adv(self.df[\"question\"][idx])\n",
    "        question_words = question_text.split(\" \")\n",
    "\n",
    "        is_positional_question = False\n",
    "\n",
    "        for i in range(self.max_question_length - len(question_words)):\n",
    "            question_vector.insert(0, 0)\n",
    "    \n",
    "        for word in question_words:\n",
    "            if not is_positional_question and (word == \"where\" or word == \"right\" or word == \"left\" or word == \"top\" or word == \"bottom\" or word == \"position\"):\n",
    "                is_positional_question = True  \n",
    "            try:\n",
    "                question[self.question2idx[word]] = 1  # one-hot表現に変換\n",
    "                question_vector.append(self.question2idx[word])\n",
    "            except KeyError:\n",
    "                question[-1] = 1  # 未知語\n",
    "                question_vector.append(0)\n",
    "        \n",
    "        # if is_positional_question:\n",
    "        #     print(f\"positional question: {question_words}\")\n",
    "        # else:\n",
    "        #     print(f\"NOT positional question: {question_words}\")\n",
    "\n",
    "        if self.answer:\n",
    "            answers_prevector = np.zeros(len(self.answer2idx))\n",
    "                \n",
    "            answers = [self.answer2idx[process_text(answer[\"answer\"])]  if process_text(answer[\"answer\"]) in self.answer2idx \n",
    "                       else 0 for answer in self.df[\"answers\"][idx]]\n",
    "            for answer in answers:\n",
    "                answers_prevector[answer] += 1\n",
    "            sum_answers = max(np.sum(answers_prevector), 1e-10)\n",
    "            answers_vector = answers_prevector / sum_answers\n",
    "            mode_answer_idx = mode(answers)  # 最頻値を取得（正解ラベル）\n",
    "\n",
    "            if is_positional_question or self.transform_adv is None:\n",
    "                image = self.transform(image)\n",
    "            else:\n",
    "                image = self.transform_adv(image)\n",
    "\n",
    "            return image, torch.Tensor(question), torch.Tensor(answers), int(mode_answer_idx), torch.Tensor(answers_vector), torch.LongTensor(question_vector)\n",
    "\n",
    "        else:\n",
    "\n",
    "            if is_positional_question or self.transform_adv is None:\n",
    "                image = self.transform(image)\n",
    "            else:\n",
    "                image = self.transform_adv(image)\n",
    "\n",
    "            return image, torch.Tensor(question), torch.LongTensor(question_vector)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadVQAData(df_path, image_dir, transform=None, answer=True):\n",
    "    return VQADataset(df_path, image_dir, transform, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 評価指標の実装\n",
    "# 簡単にするならBCEを利用する\n",
    "def VQA_criterion(batch_pred: torch.Tensor, batch_answers: torch.Tensor):\n",
    "    total_acc = 0.\n",
    "\n",
    "    for pred, answers in zip(batch_pred, batch_answers):\n",
    "        acc = 0.\n",
    "        for i in range(len(answers)):\n",
    "            num_match = 0\n",
    "            for j in range(len(answers)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if pred == answers[j]:\n",
    "                    num_match += 1\n",
    "            acc += min(num_match / 3, 1)\n",
    "        total_acc += acc / 10\n",
    "\n",
    "    return total_acc / len(batch_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. モデルのの実装\n",
    "# ResNetを利用できるようにしておく\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, layers[0], 64)\n",
    "        self.layer2 = self._make_layer(block, layers[1], 128, stride=2)\n",
    "        self.layer3 = self._make_layer(block, layers[2], 256, stride=2)\n",
    "        self.layer4 = self._make_layer(block, layers[3], 512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, 512)\n",
    "\n",
    "    def _make_layer(self, block, blocks, out_channels, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50():\n",
    "    return ResNet(BottleneckBlock, [3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet34, ResNet34_Weights, Wide_ResNet50_2_Weights, wide_resnet50_2\n",
    "\n",
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, n_answer: int):\n",
    "        super().__init__()\n",
    "        # self.resnet = ResNet18()\n",
    "        self.resnet = torchvision.models.resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "        # self.resnet = torchvision.models.wide_resnet50_2(weights=Wide_ResNet50_2_Weights.DEFAULT)\n",
    "        self.resnet.fc = nn.Linear(512, 512)\n",
    "        self.text_encoder = nn.Linear(vocab_size, 512)\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, 512, padding_idx=0)\n",
    "        # batch_first=Trueが大事！\n",
    "        self.lstm = nn.LSTM(512, 512, batch_first=True, dropout=0.20)\n",
    "\n",
    "        self.hidden2tag = nn.Linear(512, 512)\n",
    "\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, n_answer)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(512, n_answer)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, question):\n",
    "        image_feature = self.resnet(image)  # 画像の特徴量\n",
    "        # question_feature = self.text_encoder(question)  # テキストの特徴量\n",
    "\n",
    "        embeds = self.word_embeddings(question)\n",
    "        #embeds.size() = (batch_size × len(sentence) × embedding_dim)\n",
    "        _, lstm_out = self.lstm(embeds)\n",
    "        # x = torch.cat([image_feature, lstm_out[0].squeeze()], dim=1)\n",
    "\n",
    "        tag_space = self.hidden2tag(lstm_out[0])\n",
    "        question_feature = F.leaky_relu(torch.squeeze((tag_space + lstm_out[0]), dim=0))\n",
    "\n",
    "        # x = torch.cat([image_feature, tag_space.squeeze()], dim=1)\n",
    "        x = torch.cat([image_feature, question_feature], dim=1)\n",
    "\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        y = self.fc2(x)\n",
    "        x = F.leaky_relu((x + y))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=1):\n",
    "    x -= x.max(axis, keepdims=True) # expのoverflowを防ぐ\n",
    "    x_exp = np.exp(x)\n",
    "    return x_exp / x_exp.sum(axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 学習の実装\n",
    "def train_KL(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    eps = 1e-8\n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    simple_acc = 0\n",
    "\n",
    "    start = time.time()\n",
    "    for image, question, answers, mode_answer, answers_vector , question_vector in tqdm(dataloader):\n",
    "        image, question, answer, mode_answer, answers_vector, question_vector = \\\n",
    "            image.to(device), question.to(device), answers.to(device), mode_answer.to(device), answers_vector.to(device), question_vector.to(device)\n",
    "\n",
    "        pred = model(image, question_vector)\n",
    "        nn_softmax = nn.Softmax(dim=1)\n",
    "        softmax_pred = nn_softmax(pred)\n",
    "\n",
    "        loss = criterion(torch.log(softmax_pred + eps), answers_vector + eps)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\n",
    "        simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()  # simple accuracy\n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 学習の実装_normal\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    simple_acc = 0\n",
    "\n",
    "    start = time.time()\n",
    "    for image, question, answers, mode_answer, answers_vector in tqdm(dataloader):\n",
    "        image, question, answer, mode_answer, answers_vector = \\\n",
    "            image.to(device), question.to(device), answers.to(device), mode_answer.to(device), answers_vector.to(device)\n",
    "\n",
    "        pred = model(image, question)\n",
    "        loss = criterion(pred, mode_answer.squeeze())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\n",
    "        simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()  # simple accuracy\n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloader, optimizer, criterion, device):\n",
    "    model.eval()\n",
    "    eps = 1e-8\n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    simple_acc = 0\n",
    "\n",
    "    start = time.time()\n",
    "    for image, question, answers, mode_answer, answers_vector , question_vector in tqdm(dataloader):\n",
    "        image, question, answer, mode_answer, answers_vector, question_vector = \\\n",
    "            image.to(device), question.to(device), answers.to(device), mode_answer.to(device), answers_vector.to(device), question_vector.to(device)\n",
    "        \n",
    "\n",
    "        pred = model(image, question_vector)\n",
    "        nn_softmax = nn.Softmax(dim=1)\n",
    "        softmax_pred = nn_softmax(pred)\n",
    "\n",
    "        loss = criterion(torch.log(softmax_pred + eps), answers_vector + eps)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\n",
    "        simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()  # simple accuracy\n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape: (34047, 3)\n",
      "df shape: (5383, 3)\n",
      "df shape: (4969, 2)\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "# dataloader / model\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "transform_train_adv = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(degrees=(-10, 10)),\n",
    "    transforms.RandomHorizontalFlip(p=1.0),\n",
    "    transforms.RandomAffine(degrees=(-10, 10), translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset = VQADataset(df_path=\"./data/trainv7.json\", image_dir=\"./data/train\", transform=transform_train, transform_adv=transform_train_adv, answer=True)\n",
    "\n",
    "valid_dataset = VQADataset(df_path=\"./data/testv3.json\", image_dir=\"./data/train\", transform=transform_test, transform_adv=transform_test, answer=True)\n",
    "valid_dataset.update_dict(train_dataset)\n",
    "\n",
    "test_dataset = VQADataset(df_path=\"./data/valid.json\", image_dir=\"./data/valid\", transform=transform_test, answer=False)\n",
    "test_dataset.update_dict(train_dataset)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine scheduler\n",
    "class CosineScheduler:\n",
    "    def __init__(self, epochs, lr, warmup_length=5):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        epochs : int\n",
    "            学習のエポック数．\n",
    "        lr : float\n",
    "            学習率．\n",
    "        warmup_length : int\n",
    "            warmupを適用するエポック数．\n",
    "        \"\"\"\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.warmup = warmup_length\n",
    "\n",
    "    def __call__(self, epoch):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        epoch : int\n",
    "            現在のエポック数．\n",
    "        \"\"\"\n",
    "        progress = (epoch - self.warmup) / (self.epochs - self.warmup)\n",
    "        progress = np.clip(progress, 0.0, 1.0)\n",
    "        lr = self.lr * 0.5 * (1. + np.cos(np.pi * progress))\n",
    "\n",
    "        if self.warmup:\n",
    "            lr = lr * min(1., (epoch+1) / self.warmup)\n",
    "\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lr(lr, optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ATSUHIRO\\anaconda3\\envs\\dl_competition\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.15 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQAModel(\n",
      "  (resnet): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (5): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (text_encoder): Linear(in_features=5187, out_features=512, bias=True)\n",
      "  (word_embeddings): Embedding(5187, 512, padding_idx=0)\n",
      "  (lstm): LSTM(512, 512, batch_first=True, dropout=0.15)\n",
      "  (hidden2tag): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=50939, bias=True)\n",
      "  (dropout): Dropout(p=0.12, inplace=False)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (2): Linear(in_features=512, out_features=50939, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = VQAModel(vocab_size=len(train_dataset.question2idx)+1, n_answer=len(train_dataset.answer2idx)).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer / criterion\n",
    "num_epoch = 34\n",
    "warmup_length = 4\n",
    "lr = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion_KL = nn.KLDivLoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "scheduler = CosineScheduler(num_epoch, lr, warmup_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 265/266 [13:37<00:03,  3.01s/it]c:\\Users\\ATSUHIRO\\anaconda3\\envs\\dl_competition\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "100%|██████████| 266/266 [13:40<00:00,  3.09s/it]\n",
      "100%|██████████| 43/43 [01:04<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【1/28】\n",
      "train time: 820.77 [s]\n",
      "train loss: 751.0091\n",
      "train acc: 0.3614\n",
      "train simple acc: 0.2949\n",
      "train lr: 0.000250\n",
      "\n",
      "test time: 64.58 [s]\n",
      "test loss: 575.7239\n",
      "test acc: 0.2569\n",
      "test simple acc: 0.2088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:35<00:00,  2.62s/it]\n",
      "100%|██████████| 43/43 [00:51<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【2/28】\n",
      "train time: 695.80 [s]\n",
      "train loss: 621.9288\n",
      "train acc: 0.4088\n",
      "train simple acc: 0.3369\n",
      "train lr: 0.000500\n",
      "\n",
      "test time: 51.27 [s]\n",
      "test loss: 548.0985\n",
      "test acc: 0.2761\n",
      "test simple acc: 0.2257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:26<00:00,  2.58s/it]\n",
      "100%|██████████| 43/43 [00:48<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【3/28】\n",
      "train time: 686.02 [s]\n",
      "train loss: 546.9907\n",
      "train acc: 0.4275\n",
      "train simple acc: 0.3541\n",
      "train lr: 0.000750\n",
      "\n",
      "test time: 48.92 [s]\n",
      "test loss: 537.0722\n",
      "test acc: 0.3078\n",
      "test simple acc: 0.2576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:21<00:00,  2.56s/it]\n",
      "100%|██████████| 43/43 [00:48<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【4/28】\n",
      "train time: 681.06 [s]\n",
      "train loss: 482.2646\n",
      "train acc: 0.4436\n",
      "train simple acc: 0.3692\n",
      "train lr: 0.001000\n",
      "\n",
      "test time: 48.69 [s]\n",
      "test loss: 525.0414\n",
      "test acc: 0.3244\n",
      "test simple acc: 0.2669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:19<00:00,  2.55s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【5/28】\n",
      "train time: 679.54 [s]\n",
      "train loss: 411.3211\n",
      "train acc: 0.4647\n",
      "train simple acc: 0.3898\n",
      "train lr: 0.001000\n",
      "\n",
      "test time: 49.21 [s]\n",
      "test loss: 540.2909\n",
      "test acc: 0.3340\n",
      "test simple acc: 0.2763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:20<00:00,  2.56s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【6/28】\n",
      "train time: 680.78 [s]\n",
      "train loss: 347.7774\n",
      "train acc: 0.4863\n",
      "train simple acc: 0.4130\n",
      "train lr: 0.000996\n",
      "\n",
      "test time: 49.67 [s]\n",
      "test loss: 549.9727\n",
      "test acc: 0.3418\n",
      "test simple acc: 0.2843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:28<00:00,  2.59s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【7/28】\n",
      "train time: 688.12 [s]\n",
      "train loss: 289.1382\n",
      "train acc: 0.5232\n",
      "train simple acc: 0.4491\n",
      "train lr: 0.000983\n",
      "\n",
      "test time: 49.49 [s]\n",
      "test loss: 559.1536\n",
      "test acc: 0.3366\n",
      "test simple acc: 0.2788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:23<00:00,  2.57s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【8/28】\n",
      "train time: 683.09 [s]\n",
      "train loss: 240.9419\n",
      "train acc: 0.5689\n",
      "train simple acc: 0.4896\n",
      "train lr: 0.000962\n",
      "\n",
      "test time: 49.49 [s]\n",
      "test loss: 568.7937\n",
      "test acc: 0.3409\n",
      "test simple acc: 0.2815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:21<00:00,  2.56s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【9/28】\n",
      "train time: 681.12 [s]\n",
      "train loss: 204.1131\n",
      "train acc: 0.6211\n",
      "train simple acc: 0.5347\n",
      "train lr: 0.000933\n",
      "\n",
      "test time: 49.07 [s]\n",
      "test loss: 577.4087\n",
      "test acc: 0.3510\n",
      "test simple acc: 0.2927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:21<00:00,  2.56s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【10/28】\n",
      "train time: 681.10 [s]\n",
      "train loss: 179.0808\n",
      "train acc: 0.6576\n",
      "train simple acc: 0.5628\n",
      "train lr: 0.000897\n",
      "\n",
      "test time: 49.55 [s]\n",
      "test loss: 597.8725\n",
      "test acc: 0.3373\n",
      "test simple acc: 0.2777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:20<00:00,  2.56s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【11/28】\n",
      "train time: 680.86 [s]\n",
      "train loss: 156.3254\n",
      "train acc: 0.6940\n",
      "train simple acc: 0.5939\n",
      "train lr: 0.000854\n",
      "\n",
      "test time: 49.33 [s]\n",
      "test loss: 585.1074\n",
      "test acc: 0.3480\n",
      "test simple acc: 0.2855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:23<00:00,  2.57s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【12/28】\n",
      "train time: 683.61 [s]\n",
      "train loss: 140.7641\n",
      "train acc: 0.7207\n",
      "train simple acc: 0.6189\n",
      "train lr: 0.000804\n",
      "\n",
      "test time: 49.38 [s]\n",
      "test loss: 602.8645\n",
      "test acc: 0.3525\n",
      "test simple acc: 0.2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:26<00:00,  2.58s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【13/28】\n",
      "train time: 686.80 [s]\n",
      "train loss: 124.5237\n",
      "train acc: 0.7463\n",
      "train simple acc: 0.6391\n",
      "train lr: 0.000750\n",
      "\n",
      "test time: 49.53 [s]\n",
      "test loss: 601.1423\n",
      "test acc: 0.3513\n",
      "test simple acc: 0.2882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:20<00:00,  2.56s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【14/28】\n",
      "train time: 680.48 [s]\n",
      "train loss: 110.4942\n",
      "train acc: 0.7717\n",
      "train simple acc: 0.6642\n",
      "train lr: 0.000691\n",
      "\n",
      "test time: 49.48 [s]\n",
      "test loss: 604.5326\n",
      "test acc: 0.3521\n",
      "test simple acc: 0.2922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:23<00:00,  2.57s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【15/28】\n",
      "train time: 683.49 [s]\n",
      "train loss: 100.1844\n",
      "train acc: 0.7879\n",
      "train simple acc: 0.6816\n",
      "train lr: 0.000629\n",
      "\n",
      "test time: 49.49 [s]\n",
      "test loss: 593.2385\n",
      "test acc: 0.3543\n",
      "test simple acc: 0.2929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:28<00:00,  2.59s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【16/28】\n",
      "train time: 688.71 [s]\n",
      "train loss: 89.4749\n",
      "train acc: 0.8056\n",
      "train simple acc: 0.6985\n",
      "train lr: 0.000565\n",
      "\n",
      "test time: 49.93 [s]\n",
      "test loss: 606.9357\n",
      "test acc: 0.3505\n",
      "test simple acc: 0.2923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:27<00:00,  2.59s/it]\n",
      "100%|██████████| 43/43 [00:50<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【17/28】\n",
      "train time: 687.63 [s]\n",
      "train loss: 79.5203\n",
      "train acc: 0.8242\n",
      "train simple acc: 0.7188\n",
      "train lr: 0.000500\n",
      "\n",
      "test time: 50.18 [s]\n",
      "test loss: 613.8905\n",
      "test acc: 0.3569\n",
      "test simple acc: 0.2982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:19<00:00,  2.55s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【18/28】\n",
      "train time: 679.23 [s]\n",
      "train loss: 71.9261\n",
      "train acc: 0.8365\n",
      "train simple acc: 0.7333\n",
      "train lr: 0.000435\n",
      "\n",
      "test time: 49.48 [s]\n",
      "test loss: 620.1216\n",
      "test acc: 0.3579\n",
      "test simple acc: 0.2940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:11<00:00,  2.53s/it]\n",
      "100%|██████████| 43/43 [00:48<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【19/28】\n",
      "train time: 671.97 [s]\n",
      "train loss: 64.4081\n",
      "train acc: 0.8489\n",
      "train simple acc: 0.7465\n",
      "train lr: 0.000371\n",
      "\n",
      "test time: 48.96 [s]\n",
      "test loss: 633.4577\n",
      "test acc: 0.3526\n",
      "test simple acc: 0.2921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:16<00:00,  2.54s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【20/28】\n",
      "train time: 676.41 [s]\n",
      "train loss: 57.9187\n",
      "train acc: 0.8604\n",
      "train simple acc: 0.7618\n",
      "train lr: 0.000309\n",
      "\n",
      "test time: 49.28 [s]\n",
      "test loss: 622.2587\n",
      "test acc: 0.3545\n",
      "test simple acc: 0.2944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:18<00:00,  2.55s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【21/28】\n",
      "train time: 678.77 [s]\n",
      "train loss: 52.1068\n",
      "train acc: 0.8713\n",
      "train simple acc: 0.7777\n",
      "train lr: 0.000250\n",
      "\n",
      "test time: 49.41 [s]\n",
      "test loss: 632.3093\n",
      "test acc: 0.3599\n",
      "test simple acc: 0.2958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:19<00:00,  2.55s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【22/28】\n",
      "train time: 679.38 [s]\n",
      "train loss: 47.2324\n",
      "train acc: 0.8796\n",
      "train simple acc: 0.7877\n",
      "train lr: 0.000196\n",
      "\n",
      "test time: 49.33 [s]\n",
      "test loss: 636.0539\n",
      "test acc: 0.3578\n",
      "test simple acc: 0.2964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:17<00:00,  2.55s/it]\n",
      "100%|██████████| 43/43 [00:48<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【23/28】\n",
      "train time: 677.98 [s]\n",
      "train loss: 43.4182\n",
      "train acc: 0.8873\n",
      "train simple acc: 0.7982\n",
      "train lr: 0.000146\n",
      "\n",
      "test time: 48.77 [s]\n",
      "test loss: 641.7834\n",
      "test acc: 0.3544\n",
      "test simple acc: 0.2924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:14<00:00,  2.54s/it]\n",
      "100%|██████████| 43/43 [00:48<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【24/28】\n",
      "train time: 674.57 [s]\n",
      "train loss: 40.0140\n",
      "train acc: 0.8935\n",
      "train simple acc: 0.8080\n",
      "train lr: 0.000103\n",
      "\n",
      "test time: 48.99 [s]\n",
      "test loss: 644.9039\n",
      "test acc: 0.3552\n",
      "test simple acc: 0.2944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:18<00:00,  2.55s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【25/28】\n",
      "train time: 678.66 [s]\n",
      "train loss: 37.8139\n",
      "train acc: 0.8978\n",
      "train simple acc: 0.8175\n",
      "train lr: 0.000067\n",
      "\n",
      "test time: 49.36 [s]\n",
      "test loss: 647.8751\n",
      "test acc: 0.3570\n",
      "test simple acc: 0.2949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:19<00:00,  2.56s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【26/28】\n",
      "train time: 679.80 [s]\n",
      "train loss: 36.2228\n",
      "train acc: 0.8999\n",
      "train simple acc: 0.8217\n",
      "train lr: 0.000038\n",
      "\n",
      "test time: 49.63 [s]\n",
      "test loss: 651.8033\n",
      "test acc: 0.3600\n",
      "test simple acc: 0.2974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:26<00:00,  2.58s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【27/28】\n",
      "train time: 686.74 [s]\n",
      "train loss: 35.0605\n",
      "train acc: 0.9020\n",
      "train simple acc: 0.8235\n",
      "train lr: 0.000017\n",
      "\n",
      "test time: 49.44 [s]\n",
      "test loss: 652.4256\n",
      "test acc: 0.3599\n",
      "test simple acc: 0.2982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 266/266 [11:18<00:00,  2.55s/it]\n",
      "100%|██████████| 43/43 [00:49<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【28/28】\n",
      "train time: 678.23 [s]\n",
      "train loss: 34.3655\n",
      "train acc: 0.9036\n",
      "train simple acc: 0.8282\n",
      "train lr: 0.000004\n",
      "\n",
      "test time: 49.58 [s]\n",
      "test loss: 650.7091\n",
      "test acc: 0.3588\n",
      "test simple acc: 0.2962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss_hist = []\n",
    "train_acc_hist = []\n",
    "train_lr_hist = []\n",
    "\n",
    "test_loss_hist = []\n",
    "test_acc_hist = []\n",
    "\n",
    "best_test_acc = 0.\n",
    "best_epoch = 0\n",
    "\n",
    "# train model\n",
    "for epoch in range(num_epoch):\n",
    "    new_lr = scheduler(epoch)\n",
    "    set_lr(new_lr, optimizer)\n",
    "    train_lr_hist.append(new_lr)\n",
    "    \n",
    "    # train_loss, train_acc, train_simple_acc, train_time = train(model, train_loader, optimizer, criterion, device)\n",
    "    train_loss, train_acc, train_simple_acc, train_time = train_KL(model, train_loader, optimizer, criterion_KL, device)\n",
    "    test_loss, test_acc, test_simple_acc, test_time = eval(model, valid_loader, optimizer, criterion_KL, device)\n",
    "    print(f\"【{epoch + 1}/{num_epoch}】\\n\"\n",
    "            f\"train time: {train_time:.2f} [s]\\n\"\n",
    "            f\"train loss: {train_loss:.4f}\\n\"\n",
    "            f\"train acc: {train_acc:.4f}\\n\"\n",
    "            f\"train simple acc: {train_simple_acc:.4f}\\n\"\n",
    "            f\"train lr: {new_lr:.6f}\\n\")\n",
    "    print(f\"test time: {test_time:.2f} [s]\\n\"\n",
    "          f\"test loss: {test_loss:.4f}\\n\"\n",
    "          f\"test acc: {test_acc:.4f}\\n\"\n",
    "          f\"test simple acc: {test_simple_acc:.4f}\")\n",
    "    \n",
    "    if test_acc > best_test_acc:\n",
    "      best_test_acc = test_acc\n",
    "      best_epoch = epoch\n",
    "      torch.save(model.state_dict(), f\"./model_data/0709/model_{epoch}.pth\")\n",
    "    \n",
    "    train_loss_hist.append(train_loss)\n",
    "    train_acc_hist.append(train_acc)\n",
    "\n",
    "    test_loss_hist.append(test_loss)\n",
    "    test_acc_hist.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best epoch: 25\n"
     ]
    }
   ],
   "source": [
    "# modelの読み込み\n",
    "model.load_state_dict(torch.load(f\"./model_data/0709/model_{best_epoch}.pth\"))\n",
    "print(f\"best epoch: {best_epoch + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4969/4969 [02:52<00:00, 28.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# 提出用ファイルの作成\n",
    "model.eval()\n",
    "submission = []\n",
    "for image, question, question_vector in tqdm(test_loader):\n",
    "    image, question, question_vector = image.to(device), question.to(device), question_vector.to(device)\n",
    "    pred = model(image, question_vector)\n",
    "    pred = pred.argmax(1).cpu().item()\n",
    "    submission.append(pred)\n",
    "\n",
    "submission = [train_dataset.idx2answer[id] for id in submission]\n",
    "submission = np.array(submission)\n",
    "torch.save(model.state_dict(), \"model_0709.pth\")\n",
    "np.save(\"submission_0709.npy\", submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1,) and (28,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss_hist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTraining loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(num_epoch, train_acc_hist, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining and validation loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ATSUHIRO\\anaconda3\\envs\\dl_competition\\lib\\site-packages\\matplotlib\\pyplot.py:3590\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3582\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   3583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[0;32m   3584\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[1;32m-> 3590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mplot(\n\u001b[0;32m   3591\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m   3592\u001b[0m         scalex\u001b[38;5;241m=\u001b[39mscalex,\n\u001b[0;32m   3593\u001b[0m         scaley\u001b[38;5;241m=\u001b[39mscaley,\n\u001b[0;32m   3594\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m   3595\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3596\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ATSUHIRO\\anaconda3\\envs\\dl_competition\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1724\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1483\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1723\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1724\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1726\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\ATSUHIRO\\anaconda3\\envs\\dl_competition\\lib\\site-packages\\matplotlib\\axes\\_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    302\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ATSUHIRO\\anaconda3\\envs\\dl_competition\\lib\\site-packages\\matplotlib\\axes\\_base.py:499\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    496\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    500\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    503\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (28,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_epoch, train_loss_hist, 'bo', label='Training loss')\n",
    "plt.plot(num_epoch, train_acc_hist, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_epoch, train_lr_hist, 'b', label='Learning rate')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_competition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
